{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL2: Fast Reinforcement Learning via Slow Reinforcement Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a676ce28dd844e69a9e79420fa7b12a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_196ba93a359a401fb717ec3d659c3268",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5a418cfd176047838d6bf3436e0b0059",
              "IPY_MODEL_7a9fcd491077418b8888c7645dbbf10d"
            ]
          }
        },
        "196ba93a359a401fb717ec3d659c3268": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5a418cfd176047838d6bf3436e0b0059": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_c92c2fd900d4440484877d1fae12459e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_19c6ea13cdb64c6b98abd2a679ca265c"
          }
        },
        "7a9fcd491077418b8888c7645dbbf10d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8a5866a3b6f440138567a7d527cc19af",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d8e59f28b1fa4cbaa6c757b5d99c1b86"
          }
        },
        "c92c2fd900d4440484877d1fae12459e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "19c6ea13cdb64c6b98abd2a679ca265c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8a5866a3b6f440138567a7d527cc19af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d8e59f28b1fa4cbaa6c757b5d99c1b86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XDCwlc7zz5M",
        "outputId": "33c2c6b4-e1b4-4d65-f338-5bfeb05da285"
      },
      "source": [
        "# Connect to Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd drive/MyDrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZfbvCWJV_Pu"
      },
      "source": [
        "%%capture\n",
        "!pip3 install wandb -qqq"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_87UEzPRKDDs"
      },
      "source": [
        "#RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uK8x5HOeUuLb"
      },
      "source": [
        "## Utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6cyjAs4UwPI"
      },
      "source": [
        "def ema(data, alpha=0.9):\n",
        "  '''\n",
        "  Function to calculate exponential moving averages\n",
        "  Args:\n",
        "  - data : List of rewards\n",
        "  - alpha : ema factor\n",
        "  Returns\n",
        "  - averages(list) : emas of rewards\n",
        "  '''\n",
        "  averages = []\n",
        "  for i, r in enumerate(data):\n",
        "    if i == 0:\n",
        "      averages.append(r)\n",
        "    else:\n",
        "      avg = (1-alpha)*r + alpha*averages[i-1]\n",
        "      averages.append(avg)\n",
        "  return averages"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVMqOmJdKEtZ"
      },
      "source": [
        "## Tabular MDPs\n",
        "\n",
        "We use Tabular MDPs for all the experiments. All the environments have been taken from [TabulaRL](https://github.com/iosband/TabulaRL). The original paper uses the same environments for some of the experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH2Rl0ONKDfb"
      },
      "source": [
        "# Taken and adpated from https://github.com/iosband/TabulaRL/blob/master/src/environment.py\n",
        "\n",
        "import random\n",
        "\n",
        "'''\n",
        "Implementation of a basic RL environment.\n",
        "Rewards are all normal.\n",
        "Transitions are multinomial.\n",
        "author: iosband@stanford.edu\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "class Environment(object):\n",
        "    '''General RL environment'''\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def reset(self):\n",
        "        pass\n",
        "\n",
        "    def advance(self, action):\n",
        "        '''\n",
        "        Moves one step in the environment.\n",
        "        Args:\n",
        "            action\n",
        "        Returns:\n",
        "            reward - double - reward\n",
        "            newState - int - new state\n",
        "            pContinue - 0/1 - flag for end of the episode\n",
        "        '''\n",
        "        return 0, 0, 0\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "class TabularMDP(Environment):\n",
        "    '''\n",
        "    Tabular MDP\n",
        "    R - dict by (s,a) - each R[s,a] = (meanReward, sdReward)\n",
        "    P - dict by (s,a) - each P[s,a] = transition vector size S\n",
        "    '''\n",
        "\n",
        "    def __init__(self, nState, nAction, epLen):\n",
        "        '''\n",
        "        Initialize a tabular episodic MDP\n",
        "        Args:\n",
        "            nState  - int - number of states\n",
        "            nAction - int - number of actions\n",
        "            epLen   - int - episode length\n",
        "        Returns:\n",
        "            Environment object\n",
        "        '''\n",
        "\n",
        "        self.nState = nState\n",
        "        self.nAction = nAction\n",
        "        self.epLen = epLen\n",
        "\n",
        "        self.timestep = 0\n",
        "        self.state = 0\n",
        "\n",
        "        # Now initialize R and P\n",
        "        self.R = {}\n",
        "        self.P = {}\n",
        "        for state in range(nState):\n",
        "            for action in range(nAction):\n",
        "                self.R[state, action] = (1, 1)\n",
        "                self.P[state, action] = np.ones(nState) / nState\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        '''Reset the environment'''\n",
        "        self.timestep = 0\n",
        "        self.state = 0\n",
        "\n",
        "        return self.state\n",
        "\n",
        "    def advance(self, action):\n",
        "        '''\n",
        "        Move one step in the environment\n",
        "        Args:\n",
        "        action - int - chosen action\n",
        "        Returns:\n",
        "        reward - double - reward\n",
        "        newState - int - new state\n",
        "        pContinue - 0/1 - flag for end of the episode\n",
        "        '''\n",
        "        if self.R[self.state, action][1] < 1e-9:\n",
        "            # Hack for no noise\n",
        "            reward = self.R[self.state, action][0]\n",
        "        else:\n",
        "            reward = np.random.normal(loc=self.R[self.state, action][0],\n",
        "                                      scale=self.R[self.state, action][1])\n",
        "        newState = np.random.choice(self.nState, p=self.P[self.state, action])\n",
        "\n",
        "        # Update the environment\n",
        "        self.state = newState\n",
        "        self.timestep += 1\n",
        "\n",
        "        if self.timestep == self.epLen:\n",
        "            pContinue = 1\n",
        "            self.reset()\n",
        "        else:\n",
        "            pContinue = 0\n",
        "\n",
        "        return newState, reward, pContinue, None\n",
        "\n",
        "    def compute_qVals(self):\n",
        "        '''\n",
        "        Compute the Q values for the environment\n",
        "        Args:\n",
        "            NULL - works on the TabularMDP\n",
        "        Returns:\n",
        "            qVals - qVals[state, timestep] is vector of Q values for each action\n",
        "            qMax - qMax[timestep] is the vector of optimal values at timestep\n",
        "        '''\n",
        "        qVals = {}\n",
        "        qMax = {}\n",
        "\n",
        "        qMax[self.epLen] = np.zeros(self.nState)\n",
        "\n",
        "        for i in range(self.epLen):\n",
        "            j = self.epLen - i - 1\n",
        "            qMax[j] = np.zeros(self.nState)\n",
        "\n",
        "            for s in range(self.nState):\n",
        "                qVals[s, j] = np.zeros(self.nAction)\n",
        "\n",
        "                for a in range(self.nAction):\n",
        "                    qVals[s, j][a] = self.R[s, a][0] + np.dot(self.P[s, a], qMax[j + 1])\n",
        "\n",
        "                qMax[j][s] = np.max(qVals[s, j])\n",
        "        return qVals, qMax\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# Benchmark environments\n",
        "\n",
        "def make_tabularEnv(nState=10, meanReward=1, transitionParameter=1, epLen=20):\n",
        "  '''\n",
        "  Makes a Tabular MDP\n",
        "  Args:\n",
        "    nState - Number of states\n",
        "    meanReward - Mean of the reward distribution\n",
        "    transitionParameter - <>\n",
        "  Returns:\n",
        "    A regular Tabular MDP environments with rewards following a gaussian distribution \n",
        "    with a standard deviation 1 and expected reward = meanReward \n",
        "  '''\n",
        "  nAction = 2\n",
        "  R_true = {}\n",
        "  P_true = {}\n",
        "\n",
        "  for s in range(nState):\n",
        "    for a in range(nAction):\n",
        "      R_true[s, a] = (meanReward, 1)\n",
        "      P_true[s, a] = np.zeros(nState)\n",
        "\n",
        "  # Transitions\n",
        "  for s in range(nState):\n",
        "    P_true[s, 0][max(0, s-1)] = 1.\n",
        "\n",
        "  for s in range(1, nState - 1):\n",
        "    P_true[s, 1][min(nState - 1, s + 1)] = 0.35\n",
        "    P_true[s, 1][s] = 0.6\n",
        "    P_true[s, 1][max(0, s-1)] = 0.05\n",
        "\n",
        "  P_true[0, 1][0] = 0.4\n",
        "  P_true[0, 1][1] = 0.6\n",
        "  P_true[nState - 1, 1][nState - 1] = 0.6\n",
        "  P_true[nState - 1, 1][nState - 2] = 0.4\n",
        "\n",
        "  tabularEnv = TabularMDP(nState, nAction, epLen)\n",
        "  tabularEnv.R = R_true\n",
        "  tabularEnv.P = P_true\n",
        "  tabularEnv.reset()\n",
        "\n",
        "  return tabularEnv\n",
        "\n",
        "def make_riverSwim(epLen=20, nState=6):\n",
        "    '''\n",
        "    Makes the benchmark RiverSwim MDP.\n",
        "    Args:\n",
        "        NULL - works for default implementation\n",
        "    Returns:\n",
        "        riverSwim - Tabular MDP environment\n",
        "    '''\n",
        "    nAction = 2\n",
        "    R_true = {}\n",
        "    P_true = {}\n",
        "\n",
        "    for s in range(nState):\n",
        "        for a in range(nAction):\n",
        "            R_true[s, a] = (0, 0)\n",
        "            P_true[s, a] = np.zeros(nState)\n",
        "\n",
        "    # Rewards\n",
        "    r = random.uniform(1, 9)\n",
        "    R_true[0, 0] = (r / 1000, 0)\n",
        "    R_true[nState - 1, 1] = (1, 0)\n",
        "\n",
        "    # Transitions\n",
        "    for s in range(nState):\n",
        "        P_true[s, 0][max(0, s-1)] = 1.\n",
        "\n",
        "    t1 = random.uniform(0, 1)\n",
        "    t2 = random.uniform(0, 1-t1)\n",
        "    for s in range(1, nState - 1):\n",
        "        P_true[s, 1][min(nState - 1, s + 1)] = t1\n",
        "        P_true[s, 1][s] = t2\n",
        "        P_true[s, 1][max(0, s-1)] = 1-t1-t2\n",
        "\n",
        "    t3 = random.uniform(0, 1)\n",
        "    P_true[0, 1][0] = t3\n",
        "    P_true[0, 1][1] = 1 - t3\n",
        "    P_true[nState - 1, 1][nState - 1] = 1 - t3\n",
        "    P_true[nState - 1, 1][nState - 2] = t3\n",
        "\n",
        "    riverSwim = TabularMDP(nState, nAction, epLen)\n",
        "    riverSwim.R = R_true\n",
        "    riverSwim.P = P_true\n",
        "    riverSwim.reset()\n",
        "\n",
        "    return riverSwim"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z_BJnURKJCO"
      },
      "source": [
        "## Constructing the GRUPolicy\n",
        "\n",
        "The input to the policy is formed by concatenating the observation, action, reward and the episode termination flag $(s_{t+1}, a_{t}, r_{t}, d_{t})$. The observation and the actions are encoded into one hot vectors and then concatenated. This input then along with the hidden state $h_{t+1}$ produces the action $a_{t+1}$ which is then applied to the environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5fLnQGwKK_f"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "random.seed(10)\n",
        "np.random.seed(10)\n",
        "torch.manual_seed(10)\n",
        "\n",
        "class GRUPolicy(nn.Module):\n",
        "  def __init__(self, hidden_size, env):\n",
        "    super(GRUPolicy, self).__init__()\n",
        "    self.env = env\n",
        "    self.obs_embed_size = self.env.nState\n",
        "    self.action_embed_size = self.env.nAction\n",
        "    self.input_dim = self.obs_embed_size + self.action_embed_size + 1 + 1\n",
        "    self.hidden_size = hidden_size\n",
        "    self.gru = nn.GRUCell(self.input_dim, self.hidden_size, bias=True)\n",
        "    self.fc1 = nn.Linear(self.hidden_size, self.env.nAction)\n",
        "    self.reset_hidden_state()\n",
        "  \n",
        "  def make_input(self, obs, action, reward, done):\n",
        "    reward = reward.unsqueeze(1)\n",
        "    action = F.one_hot(action.long(), num_classes=self.env.nAction)\n",
        "    action = action.to(dtype).to(device)\n",
        "    done = done.unsqueeze(1)\n",
        "    input = torch.cat((obs, action, reward, done), dim=1).to(dtype).to(device)\n",
        "    return input\n",
        "\n",
        "  def reset_hidden_state(self):\n",
        "    self.h = torch.zeros(self.hidden_size).reshape(1, self.hidden_size)\n",
        "    self.h = self.h.to(dtype).to(device)\n",
        "\n",
        "  def forward(self, obs, action, reward, done, batch=False):\n",
        "    input = self.make_input(obs, action, reward, done)\n",
        "    if batch:\n",
        "      hidden = self.h.clone()\n",
        "      hidden = torch.cat(input.shape[0]*[self.h])\n",
        "      hidden = hidden.detach()\n",
        "      hidden = F.relu(self.gru(input, hidden))\n",
        "      hidden = hidden.to(dtype).to(device)\n",
        "      hidden.detach_()\n",
        "      action =self.fc1(hidden)\n",
        "      return action\n",
        "\n",
        "    self.h = F.relu(self.gru(input, self.h))\n",
        "    self.h = self.h.to(dtype).to(device)\n",
        "    self.h.detach_()\n",
        "    action = self.fc1(self.h)\n",
        "\n",
        "    return action"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpzcgZLyKNYg"
      },
      "source": [
        "## PPO with GRUPolicy\n",
        "\n",
        "We use a customm implementation of [PPO](https://spinningup.openai.com/en/latest/algorithms/ppo.html) as the RL algorithms for the inner loop. The original paper uses [TRPO](https://spinningup.openai.com/en/latest/algorithms/trpo.html) as the RL optimisation algorithm for the inner loop. These class of algorithms require relatively less hyperparameter tuning and can be used for environments with both discrete and continuous action spaces "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "id": "kH5ayB47KPaw",
        "outputId": "26ef4d93-d442-41ca-e9a5-a6891528f18e"
      },
      "source": [
        "# PPO Module adapted from https://github.com/threewisemonkeys-as/myrl/blob/master/ppo/ppo_torch.py\n",
        "\n",
        "import datetime\n",
        "import time\n",
        "from collections import deque, namedtuple\n",
        "from itertools import count\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import wandb\n",
        "!wandb.login\n",
        "\n",
        "random.seed(10)\n",
        "np.random.seed(10)\n",
        "torch.manual_seed(10)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dtype = torch.double\n",
        "\n",
        "\n",
        "class Trajectory:\n",
        "    def __init__(\n",
        "        self, observations=[], actions=[], rewards=[], dones=[], logits=[],\n",
        "    ):\n",
        "        self.obs = observations\n",
        "        self.a = actions\n",
        "        self.r = rewards\n",
        "        self.d = dones\n",
        "        self.logits = logits\n",
        "        self.len = 0\n",
        "\n",
        "    def add(\n",
        "        self,\n",
        "        obs: torch.Tensor,\n",
        "        a: torch.Tensor,\n",
        "        r: torch.Tensor,\n",
        "        d: torch.Tensor,\n",
        "        logits: torch.Tensor,\n",
        "    ):\n",
        "        self.obs.append(obs)\n",
        "        self.a.append(a)\n",
        "        self.r.append(r)\n",
        "        self.d.append(d)\n",
        "        self.logits.append(logits)\n",
        "        self.len += 1\n",
        "\n",
        "    def disc_r(self, gamma, normalize=False):\n",
        "        disc_rewards = []\n",
        "        r = 0.0\n",
        "        for reward in self.r[::-1]:\n",
        "            r = reward + gamma * r\n",
        "            disc_rewards.insert(0, r)\n",
        "        disc_rewards = torch.tensor(disc_rewards, device=device, dtype=dtype)\n",
        "        if normalize:\n",
        "            disc_rewards = (disc_rewards - disc_rewards.mean()) / (\n",
        "                disc_rewards.std() + np.finfo(np.float32).eps\n",
        "            )\n",
        "        return disc_rewards\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "# Hyperparameters\n",
        "HIDDEN_SIZE = 4\n",
        "EPOCHS = 400\n",
        "MAX_TRAJ_LENGTH = 1000\n",
        "EPISODES_PER_EPOCH = 64\n",
        "VALUE_HIDDEN_LAYERS = 32\n",
        "N_POLICY_UPDATES = 16\n",
        "N_VALUE_UPDATES = 16\n",
        "GAMMA = 0.99\n",
        "EPSILON = 0.1\n",
        "VALUE_FN_LEARNING_RATE = 1e-3\n",
        "POLICY_LEARNING_RATE = 3e-4\n",
        "\n",
        "wandb.init(project=\"ppo\", config={\n",
        "    \"trials\": 10,\n",
        "    \"hidden_size\": 4,\n",
        "    \"env_constructor\": make_riverSwim,\n",
        "    \"max_traj_length\": 1000,\n",
        "    \"epochs_per_trial\": 200,\n",
        "    \"epochs\": 700,\n",
        "    \"episodes_per_epoch\": 64,\n",
        "    \"value_hidden_layers\": 32,\n",
        "    \"n_policy_updates\": 16,\n",
        "    \"n_value_updates\": 16,\n",
        "    \"gamma\": 0.99,\n",
        "    \"epsilon\": 0.1,\n",
        "    \"value_fn_learning_rate\": 1e-3,\n",
        "    \"policy_learning_rate\": 3e-4,\n",
        "    \"nState\": 10,\n",
        "    \"epLen\": 20,\n",
        "  })\n",
        "\n",
        "config = wandb.config\n",
        "\n",
        "# Model\n",
        "class PPOGRU:\n",
        "    def __init__(\n",
        "        self,\n",
        "        env,\n",
        "        policy_hidden_layers=None,\n",
        "        value_hidden_layers=None,\n",
        "        policy=None,\n",
        "        value=None,\n",
        "    ):\n",
        "        self.env = env\n",
        "        self.env_name = 'TabularEnv'\n",
        "\n",
        "        if policy_hidden_layers is None:\n",
        "          if policy is None:\n",
        "            raise Exception(\"Unable to construct policy network!\")\n",
        "          else:\n",
        "            self.policy = policy.to(dtype).to(device)\n",
        "        else:\n",
        "          self.policy = (\n",
        "            nn.Sequential(\n",
        "                nn.Linear(env.nState, policy_hidden_layers),\n",
        "                nn.Dropout(p=0.6),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(policy_hidden_layers, env.nAction),\n",
        "            )\n",
        "            .to(device)\n",
        "            .to(dtype)\n",
        "          )\n",
        "\n",
        "        if value_hidden_layers is None:\n",
        "          if value is None:\n",
        "            raise Exception(\"Unable to construct value network!\")\n",
        "          else:\n",
        "            self.value = value\n",
        "        else:\n",
        "          self.value = (\n",
        "            nn.Sequential(\n",
        "                nn.Linear(self.env.nState, value_hidden_layers),\n",
        "                nn.Dropout(p=0.6),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(value_hidden_layers, 1),\n",
        "            )\n",
        "            .to(device)\n",
        "            .to(dtype)\n",
        "          )        \n",
        "\n",
        "    def _update(self, batch, hp, policy_optim, value_optim):\n",
        "        # process batch\n",
        "        obs = [torch.stack(traj.obs)[:-1] for traj in batch]\n",
        "        disc_r = [traj.disc_r(hp[\"gamma\"], normalize=True) for traj in batch]\n",
        "        a = [torch.stack(traj.a) for traj in batch]\n",
        "        d = [torch.stack(traj.d) for traj in batch]\n",
        "        r = [torch.stack(traj.r) for traj in batch]\n",
        "        #print(f\"Shape of a[0] : {a[0].shape}\")\n",
        "        #print(f\"Shape of d[0] : {d[0].shape}\")\n",
        "        #print(f\"Shape of obs[0] : {obs[0].shape}\")\n",
        "        #print(f\"Shape of r[0] : {r[0].shape}\")\n",
        "        with torch.no_grad():\n",
        "            v = [self.value(o) for o in obs]\n",
        "            adv = [disc_r[i] - v[i] for i in range(len(batch))]\n",
        "            old_logits = [torch.stack(traj.logits) for traj in batch]\n",
        "            old_logprobs = [\n",
        "                -F.cross_entropy(old_logits[i], a[i]) for i in range(len(batch))\n",
        "            ]\n",
        "\n",
        "        # update policy\n",
        "        for j in range(hp[\"n_policy_updates\"]):\n",
        "            policy_loss = torch.zeros(1, device=device, dtype=dtype, requires_grad=True)\n",
        "            for i, traj in enumerate(batch):\n",
        "                curr_logits = self.policy(obs[i], a[i], r[i], d[i], batch=True)\n",
        "                curr_logprobs = -F.cross_entropy(curr_logits, a[i])\n",
        "                ratio = torch.exp(curr_logprobs - old_logprobs[i])\n",
        "                clipped_ratio = torch.clamp(ratio, 1 - hp[\"epsilon\"], 1 + hp[\"epsilon\"])\n",
        "                policy_loss = (\n",
        "                    policy_loss\n",
        "                    + torch.min(ratio * adv[i], clipped_ratio * adv[i]).mean()\n",
        "                )\n",
        "\n",
        "            policy_loss = policy_loss / len(batch)\n",
        "            policy_optim.zero_grad()\n",
        "            policy_loss.backward()\n",
        "            policy_optim.step()\n",
        "\n",
        "        # update value function\n",
        "        for j in range(hp[\"n_value_updates\"]):\n",
        "            value_loss = torch.zeros(1, device=device, dtype=dtype, requires_grad=True)\n",
        "            for i in range(len(batch)):\n",
        "                v = self.value(obs[i]).view(-1)\n",
        "                value_loss = value_loss + F.mse_loss(v, disc_r[i])\n",
        "            value_loss = value_loss / len(batch)\n",
        "            value_optim.zero_grad()\n",
        "            value_loss.backward()\n",
        "            value_optim.step()\n",
        "\n",
        "        return policy_loss.item(), value_loss.item()\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        epochs=EPOCHS,\n",
        "        episodes_per_epoch=EPISODES_PER_EPOCH,\n",
        "        n_value_updates=N_VALUE_UPDATES,\n",
        "        n_policy_updates=N_POLICY_UPDATES,\n",
        "        value_lr=VALUE_FN_LEARNING_RATE,\n",
        "        policy_lr=POLICY_LEARNING_RATE,\n",
        "        gamma=GAMMA,\n",
        "        epsilon=EPSILON,\n",
        "        max_traj_length=MAX_TRAJ_LENGTH,\n",
        "        log_dir=\"./logs/\",\n",
        "        config=config,\n",
        "        RENDER=False,\n",
        "        PLOT_REWARDS=True,\n",
        "        VERBOSE=False,\n",
        "    ):\n",
        "        \"\"\" Trains both policy and value networks \"\"\"\n",
        "        hp = locals()\n",
        "        start_time = datetime.datetime.now()\n",
        "        print(\n",
        "            f\"Start time: {start_time:%d-%m-%Y %H:%M:%S}\"\n",
        "            f\"\\nTraining model on {self.env_name} | \"\n",
        "            f\"Observation Space: Discrete({self.env.nState}) | \"\n",
        "            f\"Action Space: {self.env.nAction}\\n\"\n",
        "            f\"Hyperparameters: \\n{hp}\\n\"\n",
        "        )\n",
        "        log_path = Path(log_dir)\n",
        "        log_path.mkdir(parents=True, exist_ok=False)\n",
        "\n",
        "        self.policy.train()\n",
        "        self.value.train()\n",
        "        value_optim = torch.optim.Adam(self.value.parameters(), lr=value_lr)\n",
        "        policy_optim = torch.optim.Adam(self.policy.parameters(), lr=policy_lr)\n",
        "        rewards = []\n",
        "        e = 0\n",
        "\n",
        "        try:\n",
        "            for epoch in range(epochs):\n",
        "                epoch_rewards = []\n",
        "                batch = []\n",
        "\n",
        "                # Sample trajectories\n",
        "                for e in range(episodes_per_epoch):\n",
        "                    # initialise tracking variables\n",
        "                    obs = self.env.reset()\n",
        "                    obs = torch.tensor(obs, device=device)\n",
        "                    obs = F.one_hot(obs, num_classes=self.env.nState)\n",
        "                    obs = obs.to(dtype)\n",
        "                    traj = Trajectory([obs], [], [], [], [])\n",
        "                    obs = obs.unsqueeze(0)\n",
        "                    d = torch.tensor([0], device=device, dtype=dtype)\n",
        "                    r = torch.tensor([0], device=device, dtype=dtype)\n",
        "                    a = torch.tensor([0], device=device, dtype=dtype)\n",
        "                    e += 1\n",
        "\n",
        "                    # run for single trajectory\n",
        "                    for i in range(max_traj_length):\n",
        "\n",
        "                        a_logits = self.policy(obs, a, r, d)\n",
        "                        a_logits = a_logits.reshape(self.env.nAction)\n",
        "                        a = torch.distributions.Categorical(logits=a_logits).sample()\n",
        "                        obs, r, d, _ = self.env.advance(a.item())\n",
        "\n",
        "                        obs = torch.tensor(obs, device=device)\n",
        "                        obs = F.one_hot(obs, num_classes=self.env.nState).to(device)\n",
        "                        obs = obs.to(dtype)\n",
        "                        r = torch.tensor(r, device=device, dtype=dtype)\n",
        "                        d = torch.tensor(d, device=device, dtype=dtype)\n",
        "                        traj.add(obs, a, r, d, a_logits)\n",
        "                        \n",
        "                        a = a.reshape(1)\n",
        "                        r = r.reshape(1)\n",
        "                        d = d.reshape(1)\n",
        "                        obs = obs.unsqueeze(0)\n",
        "\n",
        "                        if d:\n",
        "                            break\n",
        "\n",
        "                    epoch_rewards.append(sum(traj.r))\n",
        "                    batch.append(traj)\n",
        "\n",
        "                # Update value and policy\n",
        "                p_loss, v_loss = self._update(\n",
        "                    batch, hp, policy_optim, value_optim\n",
        "                )\n",
        "\n",
        "                # Log rewards and losses\n",
        "                epoch_rewards_tensor = torch.tensor(epoch_rewards, device=device, dtype=dtype)\n",
        "                avg_episode_reward = torch.mean(epoch_rewards_tensor[-episodes_per_epoch:])\n",
        "                rewards.append(avg_episode_reward)\n",
        "                \n",
        "                wandb.log({\n",
        "                    \"policy_loss\": p_loss,\n",
        "                    \"value_loss\": v_loss,\n",
        "                    \"reward\": avg_episode_reward,\n",
        "                    \"epoch\": epoch,\n",
        "                })\n",
        "\n",
        "                if VERBOSE and (epoch == 0 or ((epoch + 1) % (epochs / 10)) == 0):\n",
        "                    print(\n",
        "                        f\"Epoch {epoch+1}: Average Episodic Reward = {avg_episode_reward:.2f} |\"\n",
        "                        f\" Value Loss = {p_loss:.2f} |\"\n",
        "                        f\" Policy Loss = {v_loss:.2f}\"\n",
        "                    )\n",
        "\n",
        "        except KeyboardInterrupt as _:\n",
        "            print(\"\\nTraining Interrupted!\\n\")\n",
        "\n",
        "        finally:\n",
        "            #self.env.close()\n",
        "            print(\n",
        "                f\"\\nTraining Completed in {(datetime.datetime.now() - start_time).seconds} seconds\"\n",
        "            )\n",
        "            self.save(\n",
        "                log_path.joinpath(f\"{self.__class__.__name__}_{self.env_name}.pt\")\n",
        "            )\n",
        "            if PLOT_REWARDS:\n",
        "                plt.plot(rewards)\n",
        "                plt.savefig(\n",
        "                    log_path.joinpath(\n",
        "                        f\"{self.__class__.__name__}_{self.env_name}_reward_plot.png\"\n",
        "                    )\n",
        "                )\n",
        "            return rewards\n",
        "\n",
        "    def save(self, path):\n",
        "        \"\"\" Save model parameters \"\"\"\n",
        "        torch.save(\n",
        "            {\n",
        "                \"policy_state_dict\": self.policy.state_dict(),\n",
        "                \"value_state_dict\": self.value.state_dict(),\n",
        "            },\n",
        "            path,\n",
        "        )\n",
        "        print(f\"\\nSaved model parameters to {path}\")\n",
        "\n",
        "    def load(self, path=None):\n",
        "        \"\"\" Load model parameters \"\"\"\n",
        "        if path is None:\n",
        "            path = f\"./models/{self.__class__.__name__}_{self.env_name}.pt\"\n",
        "        checkpoint = torch.load(path)\n",
        "        self.policy.load_state_dict(checkpoint[\"policy_state_dict\"])\n",
        "        self.value.load_state_dict(checkpoint[\"value_state_dict\"])\n",
        "        print(f\"\\nLoaded model parameters from {path}\")\n",
        "\n",
        "    def eval(self, episodes, render=False):\n",
        "        \"\"\" Evaluates model performance \"\"\"\n",
        "\n",
        "        print(f\"\\nEvaluating model for {episodes} episodes ...\\n\")\n",
        "        start_time = datetime.datetime.now()\n",
        "        self.policy.eval()\n",
        "        rewards = []\n",
        "\n",
        "        for episode in range(episodes):\n",
        "\n",
        "            observation = self.env.reset()\n",
        "            observation = torch.tensor(observation, device=device)\n",
        "            observation = F.one_hot(observation, num_classes=self.env.nState).to(dtype)\n",
        "            observation = observation.unsqueeze(0)\n",
        "            done = torch.tensor([0], device=device, dtype=dtype)\n",
        "            reward = torch.tensor([0], device=device, dtype=dtype)\n",
        "            action = torch.tensor([0], device=device, dtype=dtype)\n",
        "            episode_rewards = []\n",
        "\n",
        "            while not done:\n",
        "                if render:\n",
        "                    self.env.render()\n",
        "\n",
        "                logits = self.policy(observation, action, reward, done)\n",
        "                logits = logits.reshape(self.env.nAction)\n",
        "                action = torch.distributions.Categorical(logits=logits).sample()\n",
        "                next_observation, reward, done, _ = self.env.advance(action.item())\n",
        "                episode_rewards.append(float(reward))\n",
        "                next_observation = torch.tensor(\n",
        "                    next_observation, device=device\n",
        "                )\n",
        "                next_observation = F.one_hot(next_observation, num_classes=self.env.nState).to(dtype)\n",
        "                observation = next_observation\n",
        "                reward = torch.tensor(reward, device=device, dtype=dtype)\n",
        "                done = torch.tensor(done, device=device, dtype=dtype)\n",
        "                action = action.reshape(1)\n",
        "                reward = reward.reshape(1)\n",
        "                done = done.reshape(1)\n",
        "                observation = observation.unsqueeze(0)\n",
        "\n",
        "            total_episode_reward = sum(episode_rewards)\n",
        "            rewards.append(total_episode_reward)\n",
        "            print(\n",
        "                f\"Episode {episode+1}: Total Episode Reward = {total_episode_reward:.2f}\"\n",
        "            )\n",
        "            rewards.append(total_episode_reward)\n",
        "            return rewards\n",
        "\n",
        "        #env.close()\n",
        "        print(f\"\\nAverage Reward for an episode = {np.mean(rewards):.2f}\")\n",
        "        print(\n",
        "            f\"Evaluation Completed in {(datetime.datetime.now() - start_time).seconds} seconds\"\n",
        "        )\n",
        "        return rewards"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: wandb.login: command not found\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.28<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">sleek-lion-26</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/veds12/ppo\" target=\"_blank\">https://wandb.ai/veds12/ppo</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/veds12/ppo/runs/3arcmt5u\" target=\"_blank\">https://wandb.ai/veds12/ppo/runs/3arcmt5u</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210503_064412-3arcmt5u</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUEZ4gC4TInK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "outputId": "581879a9-5e2c-4ed6-f638-7d71193013c1"
      },
      "source": [
        "random.seed(10)\n",
        "np.random.seed(10)\n",
        "torch.manual_seed(10)\n",
        "\n",
        "env = make_riverSwim(nState=10, epLen=20)\n",
        "policy  = GRUPolicy(hidden_size=4, env=env).to(dtype).to(device)\n",
        "model = PPOGRU(env, policy=policy, value_hidden_layers=VALUE_HIDDEN_LAYERS)\n",
        "vanilla_ppo_train_rewards = model.train(epochs=500, VERBOSE=True, PLOT_REWARDS=True, log_dir='./logs/ppo_gru_policy')\n",
        "vanilla_ppo_test_rewards = model.eval(10)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start time: 03-05-2021 06:44:35\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPOGRU object at 0x7fdd1b6bc690>, 'epochs': 500, 'episodes_per_epoch': 64, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/ppo_gru_policy', 'config': {'trials': 10, 'hidden_size': 4, 'env_constructor': '__main__.make_riverSwim', 'max_traj_length': 1000, 'epochs_per_trial': 200, 'epochs': 700, 'episodes_per_epoch': 64, 'value_hidden_layers': 32, 'n_policy_updates': 16, 'n_value_updates': 16, 'gamma': 0.99, 'epsilon': 0.1, 'value_fn_learning_rate': 0.001, 'policy_learning_rate': 0.0003, 'nState': 10, 'epLen': 20}, 'RENDER': False, 'PLOT_REWARDS': True, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = -0.03 | Policy Loss = 0.85\n",
            "Epoch 50: Average Episodic Reward = 0.04 | Value Loss = -0.00 | Policy Loss = 0.94\n",
            "Epoch 100: Average Episodic Reward = 0.06 | Value Loss = -0.03 | Policy Loss = 0.95\n",
            "Epoch 150: Average Episodic Reward = 0.07 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 200: Average Episodic Reward = 0.08 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 250: Average Episodic Reward = 0.09 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 300: Average Episodic Reward = 0.09 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 350: Average Episodic Reward = 0.10 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 400: Average Episodic Reward = 0.10 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 450: Average Episodic Reward = 0.10 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 500: Average Episodic Reward = 0.10 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 1337 seconds\n",
            "\n",
            "Saved model parameters to logs/ppo_gru_policy/PPOGRU_TabularEnv.pt\n",
            "\n",
            "Evaluating model for 10 episodes ...\n",
            "\n",
            "Episode 1: Total Episode Reward = 0.11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ge0NZWdyKea7"
      },
      "source": [
        "# RL$^2$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sggjO5VfKgsH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302,
          "referenced_widgets": [
            "0a676ce28dd844e69a9e79420fa7b12a",
            "196ba93a359a401fb717ec3d659c3268",
            "5a418cfd176047838d6bf3436e0b0059",
            "7a9fcd491077418b8888c7645dbbf10d",
            "c92c2fd900d4440484877d1fae12459e",
            "19c6ea13cdb64c6b98abd2a679ca265c",
            "8a5866a3b6f440138567a7d527cc19af",
            "d8e59f28b1fa4cbaa6c757b5d99c1b86"
          ]
        },
        "outputId": "f0b0c642-be82-42db-cdb7-a4a8b66cde97"
      },
      "source": [
        "'''\n",
        "Author: Vedant Shah\n",
        "Email: vedantshah2012@gmail.com\n",
        "'''\n",
        "\n",
        "random.seed(10)\n",
        "np.random.seed(10)\n",
        "torch.manual_seed(10)\n",
        "\n",
        "# Hyperparameters\n",
        "HIDDEN_SIZE = 4\n",
        "MAKE_ENV = make_riverSwim\n",
        "VALUE_HIDDEN_LAYERS = 32\n",
        "POLICY_HIDDEN_LAYERS = 32\n",
        "\n",
        "# Meta Train Hyperparameters\n",
        "MTR_TRIALS = 10\n",
        "MTR_MAX_TRAJ_LENGTH = 1000\n",
        "MTR_EPOCHS = 5\n",
        "MTR_EPISODES_PER_EPOCH = 2\n",
        "MTR_N_POLICY_UPDATES = 16\n",
        "MTR_N_VALUE_UPDATES = 16\n",
        "MTR_GAMMA = 0.99\n",
        "MTR_EPSILON = 0.1\n",
        "MTR_VALUE_FN_LEARNING_RATE = 1e-3\n",
        "MTR_POLICY_LEARNING_RATE = 3e-4\n",
        "\n",
        "# Meta Test Hyperparameters\n",
        "MTE_EPOCHS = 400\n",
        "MTE_EPISODES_PER_EPOCH = 64\n",
        "MTE_N_VALUE_UPDATES = 16\n",
        "MTE_N_POLICY_UPDATES = 16\n",
        "MTE_GAMMA = 0.99\n",
        "MTE_EPSILON = 0.1\n",
        "MTE_VALUE_FN_LEARNING_RATE = 1e-3\n",
        "MTE_POLICY_LEARNING_RATE = 3e-4\n",
        "MTE_MAX_TRAJ_LENGTH = 1000\n",
        "\n",
        "wandb.init(project='rl2', config={\n",
        "    'HIDDEN_SIZE': 4,\n",
        "    'MAKE_ENV': make_riverSwim,\n",
        "    'VALUE_HIDDEN_LAYERS': 32,\n",
        "    'MTR_TRIALS': 10,\n",
        "    'MTR_MAX_TRAJ_LENGTH': 1000,\n",
        "    'MTR_EPOCHS': 5,\n",
        "    'MTR_EPISODES_PER_EPOCH': 2,\n",
        "    'MTR_N_POLICY_UPDATES': 16,\n",
        "    'MTR_N_VALUE_UPDATES': 16,\n",
        "    'MTR_GAMMA': 0.99,\n",
        "    'MTR_EPSILON': 0.1,\n",
        "    'MTR_VALUE_FN_LEARNING_RATE': 1e-3,\n",
        "    'MTR_POLICY_LEARNING_RATE': 3e-4,\n",
        "    'MTE_EPOCHS': 200,\n",
        "    'MTE_EPISODES_PER_EPOCH': 4,\n",
        "    'MTE_N_VALUE_UPDATES': 16,\n",
        "    'MTE_N_POLICY_UPDATES': 16,\n",
        "    'MTE_GAMMA': 0.99,\n",
        "    'MTE_EPSILON': 0.1,\n",
        "    'MTE_VALUE_FN_LEARNING_RATE': 1e-3,\n",
        "    'MTE_POLICY_LEARNING_RATE': 3e-4,\n",
        "    'MTE_MAX_TRAJ_LENGTH': 1000\n",
        "})\n",
        "\n",
        "config = wandb.config\n",
        "\n",
        "class RL2:\n",
        "  def __init__(self,  \n",
        "               hidden_size=HIDDEN_SIZE, \n",
        "               value_hidden_layers=VALUE_HIDDEN_LAYERS, \n",
        "               policy_hidden_layers=POLICY_HIDDEN_LAYERS, \n",
        "               policy_type='recurrent', \n",
        "               make_env=MAKE_ENV, \n",
        "               log_dir=f'./logs/'):\n",
        "    \n",
        "    self.log_dir = log_dir\n",
        "    self.make_env = make_env\n",
        "    dummy_env = make_env(nState=10, epLen=20)\n",
        "    self.policy_type = policy_type\n",
        "    if policy_type == 'recurrent':\n",
        "      self.policy = GRUPolicy(hidden_size=hidden_size, env=dummy_env).to(dtype).to(device)\n",
        "    elif policy_type == 'mlp':\n",
        "      self.policy = (\n",
        "              nn.Sequential(\n",
        "                  nn.Linear(dummy_env.nState, policy_hidden_layers),\n",
        "                  nn.Dropout(p=0.6),\n",
        "                  nn.ReLU(),\n",
        "                  nn.Linear(policy_hidden_layers, dummy_env.nAction),\n",
        "              )\n",
        "              .to(device)\n",
        "              .to(dtype)\n",
        "          )\n",
        "    self.value = (\n",
        "            nn.Sequential(\n",
        "                nn.Linear(dummy_env.nState, value_hidden_layers),\n",
        "                nn.Dropout(p=0.6),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(value_hidden_layers, 1),\n",
        "            )\n",
        "            .to(device)\n",
        "            .to(dtype)\n",
        "          )\n",
        "\n",
        "  def meta_train(self, \n",
        "                 trials=MTR_TRIALS, \n",
        "                 epochs_per_trial=MTR_EPOCHS, \n",
        "                 episodes_per_epoch=MTR_EPISODES_PER_EPOCH, \n",
        "                 n_value_updates=MTR_N_VALUE_UPDATES, \n",
        "                 n_policy_updates=MTR_N_POLICY_UPDATES,\n",
        "                 value_lr=MTR_VALUE_FN_LEARNING_RATE, \n",
        "                 policy_lr=MTR_POLICY_LEARNING_RATE, \n",
        "                 gamma=MTR_GAMMA, \n",
        "                 epsilon=MTR_EPSILON, \n",
        "                 max_traj_length=MTR_MAX_TRAJ_LENGTH,\n",
        "                 reset_hidden_state=True,\n",
        "                 PLOT_REWARDS=False,\n",
        "                 VERBOSE=True):\n",
        "\n",
        "    env = self.make_env(nState=10, epLen=20)\n",
        "    if self.policy_type == 'recurrent':\n",
        "      agent = PPOGRU(env, policy=self.policy, value=self.value)\n",
        "    elif self.policy_type == 'mlp':\n",
        "      agent = PPOMLP(env, policy=self.policy, value=self.value)\n",
        "    start_time = datetime.datetime.now()\n",
        "    \n",
        "    for t in range(trials):\n",
        "      env = self.make_env(nState=10, epLen=20)\n",
        "      print(f\"\\nTRIAL NUMBER : {t+1}\\n\")\n",
        "           # f\"\\nREWARDS : {env.R} |\"\n",
        "           # f\"\\nTRANSITION PROBABILITIES : {env.P}\")\n",
        "      agent.env = env\n",
        "      if reset_hidden_state and self.policy_type == 'recurrent':\n",
        "        agent.policy.reset_hidden_state()\n",
        "      log_dir = os.path.join(self.log_dir, f'meta_train/Trial_{t+1}/')\n",
        "      _ = agent.train(\n",
        "                      epochs=epochs_per_trial,\n",
        "                      episodes_per_epoch=episodes_per_epoch,\n",
        "                      n_value_updates=n_value_updates,\n",
        "                      n_policy_updates=n_policy_updates,\n",
        "                      value_lr=value_lr,\n",
        "                      policy_lr=policy_lr,\n",
        "                      gamma=gamma,\n",
        "                      epsilon=epsilon,\n",
        "                      max_traj_length=max_traj_length,\n",
        "                      log_dir=log_dir,\n",
        "                      RENDER=False,\n",
        "                      PLOT_REWARDS=PLOT_REWARDS,\n",
        "                      VERBOSE=VERBOSE,\n",
        "                      )\n",
        "\n",
        "  def meta_test(self,\n",
        "                epochs=MTE_EPOCHS,\n",
        "                episodes_per_epoch=MTE_EPISODES_PER_EPOCH,\n",
        "                n_value_updates=MTE_N_VALUE_UPDATES,\n",
        "                n_policy_updates=MTE_N_POLICY_UPDATES,\n",
        "                value_lr=MTE_VALUE_FN_LEARNING_RATE,\n",
        "                policy_lr=MTE_POLICY_LEARNING_RATE,\n",
        "                gamma=MTE_GAMMA,\n",
        "                epsilon=MTE_EPSILON,\n",
        "                max_traj_length=MTE_MAX_TRAJ_LENGTH,\n",
        "                PLOT_REWARDS=False,\n",
        "                VERBOSE=True,\n",
        "                ):\n",
        "    env = self.make_env(nState=10, epLen=20)\n",
        "    if self.policy_type == 'recurrent':\n",
        "      agent = PPOGRU(env, policy=self.policy, value=self.value)\n",
        "      load_path = os.path.join(self.log_dir, f'meta_train/Trial_{MTR_TRIALS}/PPOGRU_TabularEnv.pt')\n",
        "    else:\n",
        "      agent = PPOMLP(env, policy=self.policy, value=self.value)\n",
        "      load_path = os.path.join(self.log_dir, f'meta_train/Trial_{MTR_TRIALS}/PPOMLP_TabularEnv.pt')\n",
        "    agent.load(path=load_path)\n",
        "    save_path = os.path.join(self.log_dir, 'meta_test')\n",
        "\n",
        "    rewards = agent.train(\n",
        "                epochs=epochs,\n",
        "                episodes_per_epoch=episodes_per_epoch,\n",
        "                n_value_updates=n_value_updates,\n",
        "                n_policy_updates=n_policy_updates,\n",
        "                value_lr=value_lr,\n",
        "                policy_lr=policy_lr,\n",
        "                gamma=gamma,\n",
        "                epsilon=epsilon,\n",
        "                max_traj_length=max_traj_length,\n",
        "                log_dir=save_path,\n",
        "                RENDER=False,\n",
        "                PLOT_REWARDS=PLOT_REWARDS,\n",
        "                VERBOSE=VERBOSE,\n",
        "                )\n",
        "    return rewards"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Finishing last run (ID:1ji2wwv9) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 1073<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a676ce28dd844e69a9e79420fa7b12a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210503_075902-1ji2wwv9/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210503_075902-1ji2wwv9/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">misty-wave-41</strong>: <a href=\"https://wandb.ai/veds12/rl2/runs/1ji2wwv9\" target=\"_blank\">https://wandb.ai/veds12/rl2/runs/1ji2wwv9</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "...Successfully finished last run (ID:1ji2wwv9). Initializing new run:<br/><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.28<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">driven-butterfly-44</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/veds12/rl2\" target=\"_blank\">https://wandb.ai/veds12/rl2</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/veds12/rl2/runs/1wccee8k\" target=\"_blank\">https://wandb.ai/veds12/rl2/runs/1wccee8k</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210503_080918-1wccee8k</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjtSIZARlcNk"
      },
      "source": [
        "### Experiment 1 : Changing the Number of Trials per MDP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUmjoCe8K7gx",
        "outputId": "0973fedb-5690-43d0-82f4-d24d264abd01"
      },
      "source": [
        "random.seed(10)\n",
        "np.random.seed(10)\n",
        "torch.manual_seed(10)\n",
        "\n",
        "rl2_0 = RL2(log_dir='./logs/RL2_Experiment_1')\n",
        "rl2_0.meta_train(trials=10)\n",
        "rl2_10_trials_rewards = rl2_0.meta_test()\n",
        "\n",
        "rl2_1 = RL2(log_dir='./logs/RL2_Experiment_2')\n",
        "rl2_1.meta_train(trials=25)\n",
        "rl2_25_trials_rewards = rl2_1.meta_test()\n",
        "\n",
        "rl2_2 = RL2(log_dir='./logs/RL2_Experiment_3')\n",
        "rl2_2.meta_train(trials=50)\n",
        "rl2_50_trials_rewards = rl2_2.meta_test()\n",
        "\n",
        "rl2_3 = RL2(log_dir='./logs/RL2_Exeriment_4')\n",
        "rl2_3.meta_train(trials=75)\n",
        "rl2_75_trials_rewards = rl2_3.meta_test()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "TRIAL NUMBER : 1\n",
            "\n",
            "Start time: 02-05-2021 14:59:35\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0090743990>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_1/meta_train/Trial_1/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.04 | Value Loss = -0.08 | Policy Loss = 1.01\n",
            "Epoch 2: Average Episodic Reward = 0.05 | Value Loss = -0.06 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.05 | Value Loss = 0.00 | Policy Loss = 0.93\n",
            "Epoch 4: Average Episodic Reward = 0.03 | Value Loss = -0.01 | Policy Loss = 1.01\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = 0.04 | Policy Loss = 1.00\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_1/meta_train/Trial_1/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 2\n",
            "\n",
            "Start time: 02-05-2021 14:59:36\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0090743990>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_1/meta_train/Trial_2/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = 0.05 | Policy Loss = 0.97\n",
            "Epoch 2: Average Episodic Reward = 0.04 | Value Loss = 0.00 | Policy Loss = 0.91\n",
            "Epoch 3: Average Episodic Reward = 0.04 | Value Loss = -0.04 | Policy Loss = 1.00\n",
            "Epoch 4: Average Episodic Reward = 0.05 | Value Loss = -0.02 | Policy Loss = 1.02\n",
            "Epoch 5: Average Episodic Reward = 0.04 | Value Loss = 0.00 | Policy Loss = 0.91\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_1/meta_train/Trial_2/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 3\n",
            "\n",
            "Start time: 02-05-2021 14:59:36\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0090743990>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_1/meta_train/Trial_3/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 0.88\n",
            "Epoch 2: Average Episodic Reward = 0.01 | Value Loss = 0.06 | Policy Loss = 0.44\n",
            "Epoch 3: Average Episodic Reward = 0.03 | Value Loss = -0.07 | Policy Loss = 0.89\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = -0.05 | Policy Loss = 0.82\n",
            "Epoch 5: Average Episodic Reward = 0.01 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_1/meta_train/Trial_3/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 4\n",
            "\n",
            "Start time: 02-05-2021 14:59:37\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0090743990>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_1/meta_train/Trial_4/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.05 | Value Loss = -0.03 | Policy Loss = 0.99\n",
            "Epoch 2: Average Episodic Reward = 0.03 | Value Loss = 0.05 | Policy Loss = 0.84\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = 0.04 | Policy Loss = 0.93\n",
            "Epoch 4: Average Episodic Reward = 0.06 | Value Loss = -0.09 | Policy Loss = 0.93\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = 0.04 | Policy Loss = 0.99\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_1/meta_train/Trial_4/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 5\n",
            "\n",
            "Start time: 02-05-2021 14:59:37\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0090743990>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_1/meta_train/Trial_5/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.00 | Value Loss = 0.27 | Policy Loss = 0.49\n",
            "Epoch 2: Average Episodic Reward = 0.03 | Value Loss = 0.03 | Policy Loss = 0.87\n",
            "Epoch 3: Average Episodic Reward = 0.00 | Value Loss = 0.08 | Policy Loss = 0.46\n",
            "Epoch 4: Average Episodic Reward = 0.00 | Value Loss = 0.07 | Policy Loss = 0.02\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = -0.04 | Policy Loss = 0.90\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_1/meta_train/Trial_5/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 6\n",
            "\n",
            "Start time: 02-05-2021 14:59:38\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0090743990>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_1/meta_train/Trial_6/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.00 | Value Loss = -0.14 | Policy Loss = 0.96\n",
            "Epoch 2: Average Episodic Reward = 0.00 | Value Loss = -0.09 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.01 | Value Loss = -0.04 | Policy Loss = 0.86\n",
            "Epoch 4: Average Episodic Reward = 0.00 | Value Loss = -0.03 | Policy Loss = 0.93\n",
            "Epoch 5: Average Episodic Reward = 0.00 | Value Loss = -0.01 | Policy Loss = 0.88\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_1/meta_train/Trial_6/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 7\n",
            "\n",
            "Start time: 02-05-2021 14:59:38\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0090743990>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_1/meta_train/Trial_7/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.01 | Value Loss = 0.09 | Policy Loss = 0.48\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = -0.04 | Policy Loss = 0.89\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = -0.08 | Policy Loss = 0.82\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 0.79\n",
            "Epoch 5: Average Episodic Reward = 0.01 | Value Loss = 0.12 | Policy Loss = 1.05\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_1/meta_train/Trial_7/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 8\n",
            "\n",
            "Start time: 02-05-2021 14:59:39\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0090743990>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_1/meta_train/Trial_8/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.04 | Value Loss = -0.19 | Policy Loss = 0.91\n",
            "Epoch 2: Average Episodic Reward = 0.03 | Value Loss = -0.11 | Policy Loss = 0.97\n",
            "Epoch 3: Average Episodic Reward = 0.04 | Value Loss = -0.06 | Policy Loss = 0.97\n",
            "Epoch 4: Average Episodic Reward = 0.04 | Value Loss = -0.03 | Policy Loss = 0.99\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = -0.04 | Policy Loss = 0.97\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_1/meta_train/Trial_8/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 9\n",
            "\n",
            "Start time: 02-05-2021 14:59:39\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0090743990>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_1/meta_train/Trial_9/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.01 | Value Loss = 0.08 | Policy Loss = 0.97\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = 0.05 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.01 | Value Loss = 0.03 | Policy Loss = 0.94\n",
            "Epoch 4: Average Episodic Reward = 0.01 | Value Loss = 0.02 | Policy Loss = 0.91\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_1/meta_train/Trial_9/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 10\n",
            "\n",
            "Start time: 02-05-2021 14:59:40\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0090743990>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_1/meta_train/Trial_10/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = -0.03 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.00 | Value Loss = 0.15 | Policy Loss = 0.53\n",
            "Epoch 3: Average Episodic Reward = 0.03 | Value Loss = 0.04 | Policy Loss = 0.93\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = -0.02 | Policy Loss = 0.97\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = 0.04 | Policy Loss = 0.46\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_1/meta_train/Trial_10/PPO_TabularEnv.pt\n",
            "\n",
            "Loaded model parameters from ./logs/RL2_Experiment_1/meta_train/Trial_10/PPO_TabularEnv.pt\n",
            "Start time: 02-05-2021 14:59:41\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0181340dd0>, 'epochs': 400, 'episodes_per_epoch': 64, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_1/meta_test', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.05 | Value Loss = -0.05 | Policy Loss = 0.95\n",
            "Epoch 40: Average Episodic Reward = 0.06 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 80: Average Episodic Reward = 0.09 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 120: Average Episodic Reward = 0.10 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 160: Average Episodic Reward = 0.12 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 200: Average Episodic Reward = 0.12 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 240: Average Episodic Reward = 0.13 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 280: Average Episodic Reward = 0.14 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 320: Average Episodic Reward = 0.14 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 360: Average Episodic Reward = 0.14 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 400: Average Episodic Reward = 0.15 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 995 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_1/meta_test/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 1\n",
            "\n",
            "Start time: 02-05-2021 15:16:16\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00902a2f50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_2/meta_train/Trial_1/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = -0.17 | Policy Loss = 0.98\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = -0.14 | Policy Loss = 1.02\n",
            "Epoch 3: Average Episodic Reward = 0.03 | Value Loss = -0.03 | Policy Loss = 1.03\n",
            "Epoch 4: Average Episodic Reward = 0.03 | Value Loss = -0.04 | Policy Loss = 0.98\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = -0.02 | Policy Loss = 0.98\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_2/meta_train/Trial_1/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 2\n",
            "\n",
            "Start time: 02-05-2021 15:16:16\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00902a2f50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_2/meta_train/Trial_2/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.01 | Value Loss = 0.02 | Policy Loss = 0.96\n",
            "Epoch 2: Average Episodic Reward = 0.01 | Value Loss = 0.00 | Policy Loss = 0.90\n",
            "Epoch 3: Average Episodic Reward = 0.01 | Value Loss = 0.03 | Policy Loss = 0.98\n",
            "Epoch 4: Average Episodic Reward = 0.01 | Value Loss = 0.01 | Policy Loss = 0.97\n",
            "Epoch 5: Average Episodic Reward = 0.01 | Value Loss = 0.00 | Policy Loss = 0.97\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_2/meta_train/Trial_2/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 3\n",
            "\n",
            "Start time: 02-05-2021 15:16:17\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00902a2f50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_2/meta_train/Trial_3/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = 0.00 | Policy Loss = 0.91\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = -0.00 | Policy Loss = 0.91\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = 0.08 | Policy Loss = 0.84\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 0.81\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = 0.00 | Policy Loss = 0.74\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_2/meta_train/Trial_3/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 4\n",
            "\n",
            "Start time: 02-05-2021 15:16:18\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00902a2f50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_2/meta_train/Trial_4/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = -0.17 | Policy Loss = 1.01\n",
            "Epoch 2: Average Episodic Reward = 0.04 | Value Loss = -0.15 | Policy Loss = 0.98\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = 0.02 | Policy Loss = 0.84\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 0.92\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = -0.06 | Policy Loss = 0.90\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_2/meta_train/Trial_4/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 5\n",
            "\n",
            "Start time: 02-05-2021 15:16:18\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00902a2f50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_2/meta_train/Trial_5/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = -0.01 | Policy Loss = 0.89\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = -0.01 | Policy Loss = 1.00\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = -0.01 | Policy Loss = 0.98\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 1.01\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_2/meta_train/Trial_5/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 6\n",
            "\n",
            "Start time: 02-05-2021 15:16:19\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00902a2f50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_2/meta_train/Trial_6/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.01 | Value Loss = 0.07 | Policy Loss = 0.96\n",
            "Epoch 2: Average Episodic Reward = 0.01 | Value Loss = 0.01 | Policy Loss = 0.88\n",
            "Epoch 3: Average Episodic Reward = 0.03 | Value Loss = -0.05 | Policy Loss = 0.97\n",
            "Epoch 4: Average Episodic Reward = 0.04 | Value Loss = -0.07 | Policy Loss = 0.91\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = 0.02 | Policy Loss = 0.99\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_2/meta_train/Trial_6/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 7\n",
            "\n",
            "Start time: 02-05-2021 15:16:19\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00902a2f50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_2/meta_train/Trial_7/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 0.94\n",
            "Epoch 2: Average Episodic Reward = 0.01 | Value Loss = 0.07 | Policy Loss = 0.79\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = -0.04 | Policy Loss = 0.99\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = -0.04 | Policy Loss = 0.90\n",
            "Epoch 5: Average Episodic Reward = 0.01 | Value Loss = 0.11 | Policy Loss = 1.01\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_2/meta_train/Trial_7/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 8\n",
            "\n",
            "Start time: 02-05-2021 15:16:20\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00902a2f50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_2/meta_train/Trial_8/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = -0.03 | Policy Loss = 1.03\n",
            "Epoch 2: Average Episodic Reward = 0.05 | Value Loss = -0.04 | Policy Loss = 0.93\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = 0.02 | Policy Loss = 0.91\n",
            "Epoch 4: Average Episodic Reward = 0.05 | Value Loss = -0.02 | Policy Loss = 0.94\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = 0.03 | Policy Loss = 0.96\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_2/meta_train/Trial_8/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 9\n",
            "\n",
            "Start time: 02-05-2021 15:16:20\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00902a2f50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_2/meta_train/Trial_9/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = -0.01 | Policy Loss = 0.94\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = 0.04 | Policy Loss = 0.97\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 0.92\n",
            "Epoch 4: Average Episodic Reward = 0.04 | Value Loss = -0.00 | Policy Loss = 0.90\n",
            "Epoch 5: Average Episodic Reward = 0.04 | Value Loss = -0.03 | Policy Loss = 1.02\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_2/meta_train/Trial_9/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 10\n",
            "\n",
            "Start time: 02-05-2021 15:16:21\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00902a2f50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_2/meta_train/Trial_10/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.05 | Value Loss = -0.05 | Policy Loss = 0.93\n",
            "Epoch 2: Average Episodic Reward = 0.04 | Value Loss = 0.01 | Policy Loss = 0.97\n",
            "Epoch 3: Average Episodic Reward = 0.04 | Value Loss = -0.01 | Policy Loss = 0.93\n",
            "Epoch 4: Average Episodic Reward = 0.04 | Value Loss = -0.03 | Policy Loss = 0.96\n",
            "Epoch 5: Average Episodic Reward = 0.05 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_2/meta_train/Trial_10/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 11\n",
            "\n",
            "Start time: 02-05-2021 15:16:21\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00902a2f50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_2/meta_train/Trial_11/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = 0.05 | Policy Loss = 0.96\n",
            "Epoch 2: Average Episodic Reward = 0.03 | Value Loss = -0.01 | Policy Loss = 0.94\n",
            "Epoch 3: Average Episodic Reward = 0.03 | Value Loss = 0.02 | Policy Loss = 0.97\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 0.93\n",
            "Epoch 5: Average Episodic Reward = 0.04 | Value Loss = -0.02 | Policy Loss = 0.94\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_2/meta_train/Trial_11/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 12\n",
            "\n",
            "Start time: 02-05-2021 15:16:22\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00902a2f50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_2/meta_train/Trial_12/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.01 | Value Loss = 0.02 | Policy Loss = 0.93\n",
            "Epoch 2: Average Episodic Reward = 0.01 | Value Loss = -0.02 | Policy Loss = 0.92\n",
            "Epoch 3: Average Episodic Reward = 0.01 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.01 | Value Loss = 0.00 | Policy Loss = 0.94\n",
            "Epoch 5: Average Episodic Reward = 0.01 | Value Loss = 0.01 | Policy Loss = 0.94\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_2/meta_train/Trial_12/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 13\n",
            "\n",
            "Start time: 02-05-2021 15:16:22\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00902a2f50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_2/meta_train/Trial_13/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.07 | Value Loss = -0.01 | Policy Loss = 0.93\n",
            "Epoch 2: Average Episodic Reward = 0.05 | Value Loss = 0.03 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.04 | Value Loss = 0.05 | Policy Loss = 0.88\n",
            "Epoch 4: Average Episodic Reward = 0.08 | Value Loss = -0.03 | Policy Loss = 0.99\n",
            "Epoch 5: Average Episodic Reward = 0.04 | Value Loss = 0.00 | Policy Loss = 0.99\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_2/meta_train/Trial_13/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 14\n",
            "\n",
            "Start time: 02-05-2021 15:16:23\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00902a2f50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_2/meta_train/Trial_14/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.05 | Value Loss = -0.04 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.05 | Value Loss = -0.01 | Policy Loss = 0.94\n",
            "Epoch 3: Average Episodic Reward = 0.03 | Value Loss = 0.02 | Policy Loss = 0.92\n",
            "Epoch 4: Average Episodic Reward = 0.05 | Value Loss = -0.03 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.04 | Value Loss = 0.00 | Policy Loss = 0.94\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_2/meta_train/Trial_14/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 15\n",
            "\n",
            "Start time: 02-05-2021 15:16:23\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00902a2f50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_2/meta_train/Trial_15/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.04 | Value Loss = 0.02 | Policy Loss = 0.93\n",
            "Epoch 2: Average Episodic Reward = 0.07 | Value Loss = -0.03 | Policy Loss = 0.94\n",
            "Epoch 3: Average Episodic Reward = 0.05 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.05 | Value Loss = 0.01 | Policy Loss = 0.93\n",
            "Epoch 5: Average Episodic Reward = 0.07 | Value Loss = 0.03 | Policy Loss = 0.94\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_2/meta_train/Trial_15/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 16\n",
            "\n",
            "Start time: 02-05-2021 15:16:24\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00902a2f50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_2/meta_train/Trial_16/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 0.94\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = 0.05 | Policy Loss = 0.97\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = 0.02 | Policy Loss = 0.89\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = -0.02 | Policy Loss = 0.93\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = -0.00 | Policy Loss = 0.97\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_2/meta_train/Trial_16/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 17\n",
            "\n",
            "Start time: 02-05-2021 15:16:25\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00902a2f50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_2/meta_train/Trial_17/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = 0.09 | Policy Loss = 0.87\n",
            "Epoch 2: Average Episodic Reward = 0.04 | Value Loss = 0.06 | Policy Loss = 0.93\n",
            "Epoch 3: Average Episodic Reward = 0.05 | Value Loss = -0.03 | Policy Loss = 0.86\n",
            "Epoch 4: Average Episodic Reward = 0.05 | Value Loss = -0.04 | Policy Loss = 0.92\n",
            "Epoch 5: Average Episodic Reward = 0.04 | Value Loss = -0.03 | Policy Loss = 0.97\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_2/meta_train/Trial_17/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 18\n",
            "\n",
            "Start time: 02-05-2021 15:16:25\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00902a2f50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_2/meta_train/Trial_18/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.07 | Value Loss = -0.09 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.08 | Value Loss = -0.06 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.54 | Value Loss = 0.10 | Policy Loss = 0.96\n",
            "Epoch 4: Average Episodic Reward = 0.04 | Value Loss = 0.00 | Policy Loss = 0.90\n",
            "Epoch 5: Average Episodic Reward = 0.06 | Value Loss = 0.08 | Policy Loss = 0.96\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_2/meta_train/Trial_18/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 19\n",
            "\n",
            "Start time: 02-05-2021 15:16:26\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00902a2f50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_2/meta_train/Trial_19/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.06 | Value Loss = 0.04 | Policy Loss = 0.84\n",
            "Epoch 2: Average Episodic Reward = 0.08 | Value Loss = -0.10 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.06 | Value Loss = -0.04 | Policy Loss = 0.82\n",
            "Epoch 4: Average Episodic Reward = 0.07 | Value Loss = -0.07 | Policy Loss = 0.96\n",
            "Epoch 5: Average Episodic Reward = 0.06 | Value Loss = 0.02 | Policy Loss = 0.88\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_2/meta_train/Trial_19/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 20\n",
            "\n",
            "Start time: 02-05-2021 15:16:26\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00902a2f50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_2/meta_train/Trial_20/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.05 | Value Loss = 0.13 | Policy Loss = 0.97\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = 0.19 | Policy Loss = 0.87\n",
            "Epoch 3: Average Episodic Reward = 0.04 | Value Loss = 0.06 | Policy Loss = 1.00\n",
            "Epoch 4: Average Episodic Reward = 0.07 | Value Loss = -0.03 | Policy Loss = 0.91\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = 0.11 | Policy Loss = 0.78\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_2/meta_train/Trial_20/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 21\n",
            "\n",
            "Start time: 02-05-2021 15:16:27\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00902a2f50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_2/meta_train/Trial_21/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.07 | Value Loss = -0.12 | Policy Loss = 0.99\n",
            "Epoch 2: Average Episodic Reward = 0.07 | Value Loss = -0.12 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.06 | Value Loss = -0.07 | Policy Loss = 0.94\n",
            "Epoch 4: Average Episodic Reward = 0.06 | Value Loss = -0.05 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.06 | Value Loss = -0.02 | Policy Loss = 0.96\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_2/meta_train/Trial_21/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 22\n",
            "\n",
            "Start time: 02-05-2021 15:16:27\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00902a2f50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_2/meta_train/Trial_22/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = -0.00 | Policy Loss = 0.92\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = 0.05 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 0.99\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = 0.00 | Policy Loss = 0.87\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = -0.02 | Policy Loss = 0.92\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_2/meta_train/Trial_22/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 23\n",
            "\n",
            "Start time: 02-05-2021 15:16:28\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00902a2f50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_2/meta_train/Trial_23/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = -0.06 | Policy Loss = 0.94\n",
            "Epoch 2: Average Episodic Reward = 0.03 | Value Loss = -0.04 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.03 | Value Loss = -0.00 | Policy Loss = 0.96\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = 0.01 | Policy Loss = 0.97\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_2/meta_train/Trial_23/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 24\n",
            "\n",
            "Start time: 02-05-2021 15:16:28\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00902a2f50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_2/meta_train/Trial_24/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = 0.06 | Policy Loss = 0.97\n",
            "Epoch 2: Average Episodic Reward = 0.07 | Value Loss = -0.02 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.04 | Value Loss = -0.01 | Policy Loss = 0.90\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = 0.05 | Policy Loss = 0.96\n",
            "Epoch 5: Average Episodic Reward = 0.04 | Value Loss = 0.02 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_2/meta_train/Trial_24/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 25\n",
            "\n",
            "Start time: 02-05-2021 15:16:29\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00902a2f50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_2/meta_train/Trial_25/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.05 | Value Loss = -0.03 | Policy Loss = 0.96\n",
            "Epoch 2: Average Episodic Reward = 0.03 | Value Loss = -0.02 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.04 | Value Loss = 0.02 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.04 | Value Loss = -0.01 | Policy Loss = 0.96\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 0.91\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_2/meta_train/Trial_25/PPO_TabularEnv.pt\n",
            "\n",
            "Loaded model parameters from ./logs/RL2_Experiment_2/meta_train/Trial_10/PPO_TabularEnv.pt\n",
            "Start time: 02-05-2021 15:16:29\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f009029a410>, 'epochs': 400, 'episodes_per_epoch': 64, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_2/meta_test', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.01 | Value Loss = 0.15 | Policy Loss = 0.85\n",
            "Epoch 40: Average Episodic Reward = 0.03 | Value Loss = -0.00 | Policy Loss = 0.93\n",
            "Epoch 80: Average Episodic Reward = 0.04 | Value Loss = 0.00 | Policy Loss = 0.94\n",
            "Epoch 120: Average Episodic Reward = 0.05 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 160: Average Episodic Reward = 0.06 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 200: Average Episodic Reward = 0.06 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 240: Average Episodic Reward = 0.06 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 280: Average Episodic Reward = 0.06 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 320: Average Episodic Reward = 0.07 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 360: Average Episodic Reward = 0.07 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 400: Average Episodic Reward = 0.07 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 990 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_2/meta_test/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 1\n",
            "\n",
            "Start time: 02-05-2021 15:33:00\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_1/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = 0.13 | Policy Loss = 0.93\n",
            "Epoch 2: Average Episodic Reward = 0.01 | Value Loss = 0.07 | Policy Loss = 0.97\n",
            "Epoch 3: Average Episodic Reward = 0.01 | Value Loss = 0.03 | Policy Loss = 0.86\n",
            "Epoch 4: Average Episodic Reward = 0.03 | Value Loss = -0.03 | Policy Loss = 0.99\n",
            "Epoch 5: Average Episodic Reward = 0.01 | Value Loss = -0.01 | Policy Loss = 0.96\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_1/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 2\n",
            "\n",
            "Start time: 02-05-2021 15:33:01\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_2/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.01 | Value Loss = -0.01 | Policy Loss = 0.96\n",
            "Epoch 2: Average Episodic Reward = 1.00 | Value Loss = 0.06 | Policy Loss = 0.48\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = -0.03 | Policy Loss = 0.91\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = -0.04 | Policy Loss = 0.99\n",
            "Epoch 5: Average Episodic Reward = 0.00 | Value Loss = 0.05 | Policy Loss = 0.46\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_2/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 3\n",
            "\n",
            "Start time: 02-05-2021 15:33:01\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_3/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = -0.04 | Policy Loss = 0.96\n",
            "Epoch 2: Average Episodic Reward = 0.03 | Value Loss = -0.05 | Policy Loss = 1.00\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = 0.05 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = 0.04 | Policy Loss = 0.49\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = 0.02 | Policy Loss = 0.91\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_3/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 4\n",
            "\n",
            "Start time: 02-05-2021 15:33:02\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_4/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = 0.01 | Policy Loss = 0.96\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = -0.02 | Policy Loss = 0.90\n",
            "Epoch 3: Average Episodic Reward = 0.01 | Value Loss = 0.03 | Policy Loss = 0.87\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = 0.02 | Policy Loss = 0.87\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = -0.02 | Policy Loss = 1.00\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_4/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 5\n",
            "\n",
            "Start time: 02-05-2021 15:33:02\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_5/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.01 | Value Loss = 0.00 | Policy Loss = 0.99\n",
            "Epoch 2: Average Episodic Reward = 0.03 | Value Loss = 0.04 | Policy Loss = 0.92\n",
            "Epoch 3: Average Episodic Reward = 0.03 | Value Loss = 0.01 | Policy Loss = 0.92\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = 0.03 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.01 | Value Loss = 0.03 | Policy Loss = 0.46\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_5/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 6\n",
            "\n",
            "Start time: 02-05-2021 15:33:03\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_6/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = -0.02 | Policy Loss = 0.47\n",
            "Epoch 2: Average Episodic Reward = 0.04 | Value Loss = -0.04 | Policy Loss = 0.86\n",
            "Epoch 3: Average Episodic Reward = 0.05 | Value Loss = -0.03 | Policy Loss = 0.80\n",
            "Epoch 4: Average Episodic Reward = 0.01 | Value Loss = 0.09 | Policy Loss = 0.88\n",
            "Epoch 5: Average Episodic Reward = 0.04 | Value Loss = -0.00 | Policy Loss = 0.65\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_6/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 7\n",
            "\n",
            "Start time: 02-05-2021 15:33:03\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_7/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.00 | Value Loss = 0.12 | Policy Loss = 0.45\n",
            "Epoch 2: Average Episodic Reward = 0.01 | Value Loss = 0.06 | Policy Loss = 0.80\n",
            "Epoch 3: Average Episodic Reward = 0.01 | Value Loss = -0.08 | Policy Loss = 0.92\n",
            "Epoch 4: Average Episodic Reward = 0.00 | Value Loss = 0.02 | Policy Loss = 1.00\n",
            "Epoch 5: Average Episodic Reward = 0.00 | Value Loss = 0.07 | Policy Loss = 0.85\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_7/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 8\n",
            "\n",
            "Start time: 02-05-2021 15:33:04\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_8/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.01 | Value Loss = -0.07 | Policy Loss = 0.79\n",
            "Epoch 2: Average Episodic Reward = 0.01 | Value Loss = -0.09 | Policy Loss = 0.87\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = -0.08 | Policy Loss = 0.91\n",
            "Epoch 4: Average Episodic Reward = 0.01 | Value Loss = -0.03 | Policy Loss = 0.91\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = -0.03 | Policy Loss = 0.94\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_8/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 9\n",
            "\n",
            "Start time: 02-05-2021 15:33:04\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_9/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = -0.01 | Policy Loss = 1.01\n",
            "Epoch 2: Average Episodic Reward = 0.01 | Value Loss = 0.04 | Policy Loss = 0.93\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = 0.04 | Policy Loss = 1.00\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = -0.02 | Policy Loss = 1.09\n",
            "Epoch 5: Average Episodic Reward = 0.01 | Value Loss = 0.02 | Policy Loss = 0.92\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_9/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 10\n",
            "\n",
            "Start time: 02-05-2021 15:33:05\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_10/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.01 | Value Loss = 0.13 | Policy Loss = 0.88\n",
            "Epoch 2: Average Episodic Reward = 0.03 | Value Loss = -0.04 | Policy Loss = 0.88\n",
            "Epoch 3: Average Episodic Reward = 0.03 | Value Loss = -0.05 | Policy Loss = 0.86\n",
            "Epoch 4: Average Episodic Reward = 0.03 | Value Loss = 0.07 | Policy Loss = 0.52\n",
            "Epoch 5: Average Episodic Reward = 0.04 | Value Loss = -0.00 | Policy Loss = 0.88\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_10/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 11\n",
            "\n",
            "Start time: 02-05-2021 15:33:05\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_11/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.04 | Value Loss = -0.07 | Policy Loss = 0.94\n",
            "Epoch 2: Average Episodic Reward = 0.04 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.04 | Value Loss = 0.01 | Policy Loss = 0.88\n",
            "Epoch 4: Average Episodic Reward = 0.06 | Value Loss = -0.04 | Policy Loss = 0.94\n",
            "Epoch 5: Average Episodic Reward = 0.07 | Value Loss = -0.04 | Policy Loss = 0.97\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_11/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 12\n",
            "\n",
            "Start time: 02-05-2021 15:33:06\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_12/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = 0.13 | Policy Loss = 0.97\n",
            "Epoch 2: Average Episodic Reward = 0.03 | Value Loss = 0.03 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.05 | Value Loss = -0.01 | Policy Loss = 0.90\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = 0.05 | Policy Loss = 0.89\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = 0.04 | Policy Loss = 0.52\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_12/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 13\n",
            "\n",
            "Start time: 02-05-2021 15:33:06\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_13/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.07 | Value Loss = -0.11 | Policy Loss = 0.88\n",
            "Epoch 2: Average Episodic Reward = 0.05 | Value Loss = -0.06 | Policy Loss = 0.91\n",
            "Epoch 3: Average Episodic Reward = 0.07 | Value Loss = -0.13 | Policy Loss = 1.01\n",
            "Epoch 4: Average Episodic Reward = 0.03 | Value Loss = 0.10 | Policy Loss = 0.47\n",
            "Epoch 5: Average Episodic Reward = 0.06 | Value Loss = -0.00 | Policy Loss = 0.82\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_13/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 14\n",
            "\n",
            "Start time: 02-05-2021 15:33:07\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_14/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.01 | Value Loss = 0.18 | Policy Loss = 0.40\n",
            "Epoch 2: Average Episodic Reward = 1.02 | Value Loss = 0.05 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 0.84\n",
            "Epoch 4: Average Episodic Reward = 0.01 | Value Loss = 0.07 | Policy Loss = 0.96\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = -0.02 | Policy Loss = 0.52\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_14/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 15\n",
            "\n",
            "Start time: 02-05-2021 15:33:08\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_15/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.05 | Value Loss = -0.26 | Policy Loss = 0.96\n",
            "Epoch 2: Average Episodic Reward = 0.05 | Value Loss = -0.19 | Policy Loss = 0.98\n",
            "Epoch 3: Average Episodic Reward = 0.06 | Value Loss = -0.14 | Policy Loss = 0.98\n",
            "Epoch 4: Average Episodic Reward = 0.07 | Value Loss = -0.08 | Policy Loss = 0.94\n",
            "Epoch 5: Average Episodic Reward = 0.06 | Value Loss = -0.02 | Policy Loss = 0.99\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_15/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 16\n",
            "\n",
            "Start time: 02-05-2021 15:33:08\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_16/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = 0.02 | Policy Loss = 0.90\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = 0.02 | Policy Loss = 0.89\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = 0.07 | Policy Loss = 0.94\n",
            "Epoch 4: Average Episodic Reward = 0.03 | Value Loss = -0.03 | Policy Loss = 0.97\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = 0.05 | Policy Loss = 0.92\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_16/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 17\n",
            "\n",
            "Start time: 02-05-2021 15:33:09\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_17/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.01 | Value Loss = 0.08 | Policy Loss = 0.81\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = 0.05 | Policy Loss = 0.85\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 0.99\n",
            "Epoch 4: Average Episodic Reward = 0.01 | Value Loss = 0.03 | Policy Loss = 0.86\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = -0.10 | Policy Loss = 0.93\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_17/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 18\n",
            "\n",
            "Start time: 02-05-2021 15:33:09\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_18/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.04 | Value Loss = -0.03 | Policy Loss = 1.01\n",
            "Epoch 2: Average Episodic Reward = 0.03 | Value Loss = -0.07 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.07 | Value Loss = -0.00 | Policy Loss = 1.03\n",
            "Epoch 4: Average Episodic Reward = 0.08 | Value Loss = -0.03 | Policy Loss = 0.86\n",
            "Epoch 5: Average Episodic Reward = 0.04 | Value Loss = 0.03 | Policy Loss = 1.07\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_18/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 19\n",
            "\n",
            "Start time: 02-05-2021 15:33:10\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_19/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.04 | Value Loss = -0.04 | Policy Loss = 0.96\n",
            "Epoch 2: Average Episodic Reward = 0.04 | Value Loss = -0.01 | Policy Loss = 0.93\n",
            "Epoch 3: Average Episodic Reward = 0.05 | Value Loss = -0.02 | Policy Loss = 0.96\n",
            "Epoch 4: Average Episodic Reward = 0.03 | Value Loss = 0.04 | Policy Loss = 0.50\n",
            "Epoch 5: Average Episodic Reward = 0.04 | Value Loss = -0.02 | Policy Loss = 0.94\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_19/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 20\n",
            "\n",
            "Start time: 02-05-2021 15:33:10\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_20/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.08 | Value Loss = -0.03 | Policy Loss = 0.96\n",
            "Epoch 2: Average Episodic Reward = 0.07 | Value Loss = -0.01 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.07 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.06 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.06 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_20/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 21\n",
            "\n",
            "Start time: 02-05-2021 15:33:11\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_21/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = 0.03 | Policy Loss = 0.94\n",
            "Epoch 2: Average Episodic Reward = 0.01 | Value Loss = 0.10 | Policy Loss = 0.85\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = 0.03 | Policy Loss = 1.01\n",
            "Epoch 4: Average Episodic Reward = 0.01 | Value Loss = 0.00 | Policy Loss = 0.91\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = -0.03 | Policy Loss = 0.93\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_21/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 22\n",
            "\n",
            "Start time: 02-05-2021 15:33:11\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_22/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = -0.01 | Policy Loss = 0.98\n",
            "Epoch 2: Average Episodic Reward = 0.04 | Value Loss = -0.02 | Policy Loss = 0.94\n",
            "Epoch 3: Average Episodic Reward = 0.05 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.05 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.04 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_22/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 23\n",
            "\n",
            "Start time: 02-05-2021 15:33:12\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_23/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = 0.02 | Policy Loss = 0.96\n",
            "Epoch 2: Average Episodic Reward = 0.01 | Value Loss = 0.00 | Policy Loss = 0.93\n",
            "Epoch 3: Average Episodic Reward = 0.01 | Value Loss = 0.02 | Policy Loss = 0.89\n",
            "Epoch 4: Average Episodic Reward = 0.01 | Value Loss = -0.05 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.01 | Value Loss = 0.02 | Policy Loss = 0.94\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_23/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 24\n",
            "\n",
            "Start time: 02-05-2021 15:33:12\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_24/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.01 | Value Loss = 0.02 | Policy Loss = 0.87\n",
            "Epoch 2: Average Episodic Reward = 0.03 | Value Loss = 0.00 | Policy Loss = 0.91\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = 0.02 | Policy Loss = 0.52\n",
            "Epoch 4: Average Episodic Reward = 0.03 | Value Loss = -0.06 | Policy Loss = 0.93\n",
            "Epoch 5: Average Episodic Reward = 0.01 | Value Loss = 0.10 | Policy Loss = 0.42\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_24/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 25\n",
            "\n",
            "Start time: 02-05-2021 15:33:13\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_25/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.04 | Value Loss = -0.11 | Policy Loss = 0.96\n",
            "Epoch 2: Average Episodic Reward = 0.03 | Value Loss = -0.07 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.04 | Value Loss = -0.04 | Policy Loss = 0.93\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = -0.02 | Policy Loss = 0.97\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_25/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 26\n",
            "\n",
            "Start time: 02-05-2021 15:33:13\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_26/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.06 | Value Loss = -0.01 | Policy Loss = 0.93\n",
            "Epoch 2: Average Episodic Reward = 0.06 | Value Loss = -0.01 | Policy Loss = 0.94\n",
            "Epoch 3: Average Episodic Reward = 0.06 | Value Loss = -0.00 | Policy Loss = 0.97\n",
            "Epoch 4: Average Episodic Reward = 0.07 | Value Loss = -0.01 | Policy Loss = 0.93\n",
            "Epoch 5: Average Episodic Reward = 0.05 | Value Loss = 0.02 | Policy Loss = 0.97\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_26/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 27\n",
            "\n",
            "Start time: 02-05-2021 15:33:14\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_27/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = 0.06 | Policy Loss = 0.88\n",
            "Epoch 2: Average Episodic Reward = 0.03 | Value Loss = -0.01 | Policy Loss = 0.92\n",
            "Epoch 3: Average Episodic Reward = 0.03 | Value Loss = -0.01 | Policy Loss = 0.94\n",
            "Epoch 4: Average Episodic Reward = 0.03 | Value Loss = 0.00 | Policy Loss = 0.94\n",
            "Epoch 5: Average Episodic Reward = 0.01 | Value Loss = 0.08 | Policy Loss = 0.92\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_27/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 28\n",
            "\n",
            "Start time: 02-05-2021 15:33:15\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_28/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = 0.00 | Policy Loss = 0.96\n",
            "Epoch 2: Average Episodic Reward = 0.04 | Value Loss = -0.04 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.03 | Value Loss = 0.04 | Policy Loss = 0.99\n",
            "Epoch 4: Average Episodic Reward = 0.03 | Value Loss = 0.04 | Policy Loss = 0.93\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = 0.01 | Policy Loss = 0.92\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_28/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 29\n",
            "\n",
            "Start time: 02-05-2021 15:33:15\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_29/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = -0.03 | Policy Loss = 0.96\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = -0.01 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.03 | Value Loss = -0.01 | Policy Loss = 0.96\n",
            "Epoch 4: Average Episodic Reward = 0.01 | Value Loss = 0.01 | Policy Loss = 0.97\n",
            "Epoch 5: Average Episodic Reward = 0.01 | Value Loss = 0.03 | Policy Loss = 0.96\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_29/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 30\n",
            "\n",
            "Start time: 02-05-2021 15:33:16\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_30/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.05 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.06 | Value Loss = 0.00 | Policy Loss = 0.92\n",
            "Epoch 3: Average Episodic Reward = 0.06 | Value Loss = 0.00 | Policy Loss = 0.97\n",
            "Epoch 4: Average Episodic Reward = 0.07 | Value Loss = -0.00 | Policy Loss = 0.94\n",
            "Epoch 5: Average Episodic Reward = 0.10 | Value Loss = -0.00 | Policy Loss = 0.94\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_30/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 31\n",
            "\n",
            "Start time: 02-05-2021 15:33:16\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_31/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.06 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.07 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.06 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.07 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.06 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_31/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 32\n",
            "\n",
            "Start time: 02-05-2021 15:33:17\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_32/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = 0.00 | Policy Loss = 0.96\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.04 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.03 | Value Loss = 0.00 | Policy Loss = 0.93\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 0.92\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_32/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 33\n",
            "\n",
            "Start time: 02-05-2021 15:33:17\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_33/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = -0.04 | Policy Loss = 0.93\n",
            "Epoch 2: Average Episodic Reward = 0.01 | Value Loss = -0.01 | Policy Loss = 0.94\n",
            "Epoch 3: Average Episodic Reward = 0.01 | Value Loss = 0.03 | Policy Loss = 0.98\n",
            "Epoch 4: Average Episodic Reward = 0.01 | Value Loss = 0.01 | Policy Loss = 0.92\n",
            "Epoch 5: Average Episodic Reward = 0.01 | Value Loss = 0.01 | Policy Loss = 0.91\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_33/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 34\n",
            "\n",
            "Start time: 02-05-2021 15:33:18\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_34/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.06 | Value Loss = -0.09 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.05 | Value Loss = -0.04 | Policy Loss = 0.97\n",
            "Epoch 3: Average Episodic Reward = 0.06 | Value Loss = -0.01 | Policy Loss = 0.96\n",
            "Epoch 4: Average Episodic Reward = 0.07 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.08 | Value Loss = 0.01 | Policy Loss = 0.96\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_34/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 35\n",
            "\n",
            "Start time: 02-05-2021 15:33:18\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_35/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.05 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.05 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = 0.02 | Policy Loss = 0.97\n",
            "Epoch 4: Average Episodic Reward = 0.03 | Value Loss = 0.02 | Policy Loss = 0.47\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_35/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 36\n",
            "\n",
            "Start time: 02-05-2021 15:33:19\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_36/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = -0.02 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.01 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.01 | Value Loss = 0.01 | Policy Loss = 0.93\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = -0.01 | Policy Loss = 0.96\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_36/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 37\n",
            "\n",
            "Start time: 02-05-2021 15:33:19\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_37/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.07 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.05 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.05 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.05 | Value Loss = 0.01 | Policy Loss = 0.96\n",
            "Epoch 5: Average Episodic Reward = 0.05 | Value Loss = -0.01 | Policy Loss = 0.94\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_37/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 38\n",
            "\n",
            "Start time: 02-05-2021 15:33:20\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_38/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.01 | Value Loss = 0.05 | Policy Loss = 0.49\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = -0.01 | Policy Loss = 0.93\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = -0.03 | Policy Loss = 0.96\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_38/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 39\n",
            "\n",
            "Start time: 02-05-2021 15:33:21\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_39/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.11 | Value Loss = -0.02 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.09 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.11 | Value Loss = 0.01 | Policy Loss = 0.92\n",
            "Epoch 4: Average Episodic Reward = 0.05 | Value Loss = 0.02 | Policy Loss = 0.91\n",
            "Epoch 5: Average Episodic Reward = 0.08 | Value Loss = -0.02 | Policy Loss = 0.92\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_39/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 40\n",
            "\n",
            "Start time: 02-05-2021 15:33:21\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_40/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.01 | Value Loss = -0.03 | Policy Loss = 0.94\n",
            "Epoch 2: Average Episodic Reward = 0.01 | Value Loss = -0.00 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.01 | Value Loss = 0.00 | Policy Loss = 0.96\n",
            "Epoch 4: Average Episodic Reward = 0.01 | Value Loss = 0.02 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.01 | Value Loss = 0.00 | Policy Loss = 0.94\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_40/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 41\n",
            "\n",
            "Start time: 02-05-2021 15:33:22\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_41/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.07 | Value Loss = 0.00 | Policy Loss = 0.94\n",
            "Epoch 2: Average Episodic Reward = 0.07 | Value Loss = 0.01 | Policy Loss = 0.94\n",
            "Epoch 3: Average Episodic Reward = 0.09 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.09 | Value Loss = 0.00 | Policy Loss = 0.96\n",
            "Epoch 5: Average Episodic Reward = 0.08 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_41/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 42\n",
            "\n",
            "Start time: 02-05-2021 15:33:22\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_42/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 0.94\n",
            "Epoch 2: Average Episodic Reward = 0.03 | Value Loss = -0.00 | Policy Loss = 0.94\n",
            "Epoch 3: Average Episodic Reward = 0.03 | Value Loss = -0.00 | Policy Loss = 0.93\n",
            "Epoch 4: Average Episodic Reward = 0.03 | Value Loss = 0.00 | Policy Loss = 0.93\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = -0.01 | Policy Loss = 0.96\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_42/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 43\n",
            "\n",
            "Start time: 02-05-2021 15:33:23\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_43/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = 0.05 | Policy Loss = 1.01\n",
            "Epoch 2: Average Episodic Reward = 0.03 | Value Loss = 0.05 | Policy Loss = 0.97\n",
            "Epoch 3: Average Episodic Reward = 0.03 | Value Loss = -0.00 | Policy Loss = 0.94\n",
            "Epoch 4: Average Episodic Reward = 0.03 | Value Loss = 0.01 | Policy Loss = 0.96\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = 0.01 | Policy Loss = 0.96\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_43/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 44\n",
            "\n",
            "Start time: 02-05-2021 15:33:23\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_44/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = 0.03 | Policy Loss = 0.94\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = 0.03 | Policy Loss = 0.93\n",
            "Epoch 3: Average Episodic Reward = 0.04 | Value Loss = 0.01 | Policy Loss = 0.90\n",
            "Epoch 4: Average Episodic Reward = 0.03 | Value Loss = -0.01 | Policy Loss = 0.94\n",
            "Epoch 5: Average Episodic Reward = 0.05 | Value Loss = -0.04 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_44/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 45\n",
            "\n",
            "Start time: 02-05-2021 15:33:24\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_45/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = -0.05 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = -0.02 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.01 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_45/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 46\n",
            "\n",
            "Start time: 02-05-2021 15:33:24\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_46/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = 0.03 | Policy Loss = 0.92\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = -0.02 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = 0.03 | Policy Loss = 0.87\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = -0.02 | Policy Loss = 0.94\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 1.00\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_46/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 47\n",
            "\n",
            "Start time: 02-05-2021 15:33:25\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_47/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.08 | Value Loss = -0.00 | Policy Loss = 0.93\n",
            "Epoch 2: Average Episodic Reward = 0.07 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.06 | Value Loss = 0.02 | Policy Loss = 0.86\n",
            "Epoch 4: Average Episodic Reward = 0.04 | Value Loss = 0.01 | Policy Loss = 0.93\n",
            "Epoch 5: Average Episodic Reward = 0.05 | Value Loss = 0.01 | Policy Loss = 0.91\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_47/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 48\n",
            "\n",
            "Start time: 02-05-2021 15:33:25\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_48/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = -0.04 | Policy Loss = 0.97\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = -0.04 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.01 | Value Loss = 0.04 | Policy Loss = 0.96\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = -0.02 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = -0.01 | Policy Loss = 0.93\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_48/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 49\n",
            "\n",
            "Start time: 02-05-2021 15:33:26\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_49/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.01 | Value Loss = 0.02 | Policy Loss = 0.91\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = -0.03 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = -0.02 | Policy Loss = 0.96\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = -0.00 | Policy Loss = 0.92\n",
            "Epoch 5: Average Episodic Reward = 0.01 | Value Loss = 0.01 | Policy Loss = 0.97\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_49/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 50\n",
            "\n",
            "Start time: 02-05-2021 15:33:27\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0091793190>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_train/Trial_50/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.04 | Value Loss = 0.02 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.06 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.05 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.04 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.04 | Value Loss = 0.01 | Policy Loss = 0.94\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_train/Trial_50/PPO_TabularEnv.pt\n",
            "\n",
            "Loaded model parameters from ./logs/RL2_Experiment_3/meta_train/Trial_10/PPO_TabularEnv.pt\n",
            "Start time: 02-05-2021 15:33:27\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917f70d0>, 'epochs': 400, 'episodes_per_epoch': 64, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_3/meta_test', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = -0.06 | Policy Loss = 0.96\n",
            "Epoch 40: Average Episodic Reward = 0.04 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 80: Average Episodic Reward = 0.04 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 120: Average Episodic Reward = 0.05 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 160: Average Episodic Reward = 0.06 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 200: Average Episodic Reward = 0.07 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 240: Average Episodic Reward = 0.07 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 280: Average Episodic Reward = 0.07 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 320: Average Episodic Reward = 0.08 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 360: Average Episodic Reward = 0.08 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 400: Average Episodic Reward = 0.09 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 994 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_3/meta_test/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 1\n",
            "\n",
            "Start time: 02-05-2021 15:50:01\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_1/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.05 | Value Loss = -0.06 | Policy Loss = 1.04\n",
            "Epoch 2: Average Episodic Reward = 0.05 | Value Loss = 0.02 | Policy Loss = 0.98\n",
            "Epoch 3: Average Episodic Reward = 0.05 | Value Loss = -0.01 | Policy Loss = 0.94\n",
            "Epoch 4: Average Episodic Reward = 0.06 | Value Loss = -0.02 | Policy Loss = 0.98\n",
            "Epoch 5: Average Episodic Reward = 0.06 | Value Loss = -0.02 | Policy Loss = 0.89\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_1/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 2\n",
            "\n",
            "Start time: 02-05-2021 15:50:02\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_2/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.04 | Value Loss = 0.02 | Policy Loss = 0.90\n",
            "Epoch 2: Average Episodic Reward = 0.05 | Value Loss = -0.02 | Policy Loss = 1.01\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = 0.06 | Policy Loss = 0.44\n",
            "Epoch 4: Average Episodic Reward = 0.05 | Value Loss = 0.01 | Policy Loss = 0.89\n",
            "Epoch 5: Average Episodic Reward = 0.05 | Value Loss = -0.02 | Policy Loss = 1.02\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_2/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 3\n",
            "\n",
            "Start time: 02-05-2021 15:50:02\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_3/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.01 | Value Loss = 0.02 | Policy Loss = 1.04\n",
            "Epoch 2: Average Episodic Reward = 0.01 | Value Loss = 0.01 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = -0.02 | Policy Loss = 0.92\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = -0.01 | Policy Loss = 0.90\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = -0.01 | Policy Loss = 0.94\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_3/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 4\n",
            "\n",
            "Start time: 02-05-2021 15:50:03\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_4/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.04 | Value Loss = 0.04 | Policy Loss = 0.93\n",
            "Epoch 2: Average Episodic Reward = 0.05 | Value Loss = -0.01 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.05 | Value Loss = -0.03 | Policy Loss = 0.94\n",
            "Epoch 4: Average Episodic Reward = 0.06 | Value Loss = -0.01 | Policy Loss = 0.97\n",
            "Epoch 5: Average Episodic Reward = 0.01 | Value Loss = 0.01 | Policy Loss = 0.97\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_4/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 5\n",
            "\n",
            "Start time: 02-05-2021 15:50:03\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_5/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = -0.02 | Policy Loss = 0.96\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = -0.04 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.03 | Value Loss = 0.01 | Policy Loss = 0.92\n",
            "Epoch 5: Average Episodic Reward = 0.04 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_5/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 6\n",
            "\n",
            "Start time: 02-05-2021 15:50:04\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_6/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.01 | Value Loss = 0.00 | Policy Loss = 0.90\n",
            "Epoch 2: Average Episodic Reward = 0.01 | Value Loss = -0.02 | Policy Loss = 0.92\n",
            "Epoch 3: Average Episodic Reward = 0.01 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.01 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.01 | Value Loss = 0.05 | Policy Loss = 0.89\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_6/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 7\n",
            "\n",
            "Start time: 02-05-2021 15:50:04\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_7/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.06 | Value Loss = 0.01 | Policy Loss = 0.89\n",
            "Epoch 2: Average Episodic Reward = 0.04 | Value Loss = 0.03 | Policy Loss = 0.82\n",
            "Epoch 3: Average Episodic Reward = 0.06 | Value Loss = -0.01 | Policy Loss = 0.96\n",
            "Epoch 4: Average Episodic Reward = 0.04 | Value Loss = -0.00 | Policy Loss = 0.84\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = 0.11 | Policy Loss = 0.89\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_7/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 8\n",
            "\n",
            "Start time: 02-05-2021 15:50:05\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_8/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = -0.12 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.01 | Value Loss = 0.03 | Policy Loss = 0.98\n",
            "Epoch 3: Average Episodic Reward = 0.01 | Value Loss = 0.00 | Policy Loss = 0.99\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = -0.05 | Policy Loss = 0.94\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = -0.01 | Policy Loss = 0.89\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_8/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 9\n",
            "\n",
            "Start time: 02-05-2021 15:50:06\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_9/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = -0.01 | Policy Loss = 0.94\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = 0.03 | Policy Loss = 0.93\n",
            "Epoch 3: Average Episodic Reward = 0.04 | Value Loss = -0.05 | Policy Loss = 0.96\n",
            "Epoch 4: Average Episodic Reward = 0.03 | Value Loss = -0.02 | Policy Loss = 0.94\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 0.94\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_9/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 10\n",
            "\n",
            "Start time: 02-05-2021 15:50:06\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_10/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = 0.09 | Policy Loss = 0.46\n",
            "Epoch 2: Average Episodic Reward = 0.04 | Value Loss = 0.02 | Policy Loss = 0.99\n",
            "Epoch 3: Average Episodic Reward = 0.03 | Value Loss = -0.01 | Policy Loss = 0.98\n",
            "Epoch 4: Average Episodic Reward = 0.04 | Value Loss = -0.00 | Policy Loss = 0.90\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = 0.01 | Policy Loss = 0.46\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_10/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 11\n",
            "\n",
            "Start time: 02-05-2021 15:50:07\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_11/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.06 | Value Loss = -0.05 | Policy Loss = 0.96\n",
            "Epoch 2: Average Episodic Reward = 0.04 | Value Loss = -0.00 | Policy Loss = 0.94\n",
            "Epoch 3: Average Episodic Reward = 0.05 | Value Loss = -0.01 | Policy Loss = 0.97\n",
            "Epoch 4: Average Episodic Reward = 0.04 | Value Loss = 0.01 | Policy Loss = 0.93\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = 0.02 | Policy Loss = 0.93\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_11/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 12\n",
            "\n",
            "Start time: 02-05-2021 15:50:07\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_12/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.01 | Value Loss = 0.04 | Policy Loss = 0.97\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = -0.02 | Policy Loss = 0.92\n",
            "Epoch 3: Average Episodic Reward = 0.03 | Value Loss = -0.02 | Policy Loss = 0.94\n",
            "Epoch 4: Average Episodic Reward = 0.04 | Value Loss = -0.03 | Policy Loss = 0.94\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = 0.02 | Policy Loss = 0.90\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_12/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 13\n",
            "\n",
            "Start time: 02-05-2021 15:50:08\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_13/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.04 | Value Loss = -0.04 | Policy Loss = 0.94\n",
            "Epoch 2: Average Episodic Reward = 0.03 | Value Loss = -0.02 | Policy Loss = 0.94\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = 0.03 | Policy Loss = 0.98\n",
            "Epoch 4: Average Episodic Reward = 0.03 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.04 | Value Loss = -0.01 | Policy Loss = 0.94\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_13/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 14\n",
            "\n",
            "Start time: 02-05-2021 15:50:08\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_14/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.06 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.07 | Value Loss = 0.00 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.06 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.08 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.07 | Value Loss = -0.00 | Policy Loss = 0.94\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_14/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 15\n",
            "\n",
            "Start time: 02-05-2021 15:50:09\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_15/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.05 | Value Loss = -0.00 | Policy Loss = 0.96\n",
            "Epoch 2: Average Episodic Reward = 0.03 | Value Loss = 0.02 | Policy Loss = 0.86\n",
            "Epoch 3: Average Episodic Reward = 0.05 | Value Loss = -0.02 | Policy Loss = 0.94\n",
            "Epoch 4: Average Episodic Reward = 0.04 | Value Loss = 0.01 | Policy Loss = 1.01\n",
            "Epoch 5: Average Episodic Reward = 0.06 | Value Loss = -0.02 | Policy Loss = 0.96\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_15/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 16\n",
            "\n",
            "Start time: 02-05-2021 15:50:09\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_16/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.07 | Value Loss = 0.01 | Policy Loss = 0.94\n",
            "Epoch 2: Average Episodic Reward = 0.07 | Value Loss = 0.01 | Policy Loss = 0.91\n",
            "Epoch 3: Average Episodic Reward = 0.07 | Value Loss = 0.01 | Policy Loss = 0.90\n",
            "Epoch 4: Average Episodic Reward = 0.07 | Value Loss = 0.01 | Policy Loss = 0.92\n",
            "Epoch 5: Average Episodic Reward = 0.07 | Value Loss = 0.01 | Policy Loss = 1.02\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_16/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 17\n",
            "\n",
            "Start time: 02-05-2021 15:50:10\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_17/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.04 | Value Loss = 0.03 | Policy Loss = 0.98\n",
            "Epoch 2: Average Episodic Reward = 0.05 | Value Loss = -0.06 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.05 | Value Loss = -0.02 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.04 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.06 | Value Loss = 0.00 | Policy Loss = 0.92\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_17/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 18\n",
            "\n",
            "Start time: 02-05-2021 15:50:10\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_18/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.05 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.03 | Value Loss = 0.03 | Policy Loss = 0.94\n",
            "Epoch 3: Average Episodic Reward = 0.05 | Value Loss = -0.01 | Policy Loss = 0.97\n",
            "Epoch 4: Average Episodic Reward = 0.04 | Value Loss = -0.01 | Policy Loss = 0.96\n",
            "Epoch 5: Average Episodic Reward = 0.04 | Value Loss = -0.00 | Policy Loss = 0.96\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_18/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 19\n",
            "\n",
            "Start time: 02-05-2021 15:50:11\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_19/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = 0.01 | Policy Loss = 0.94\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = -0.01 | Policy Loss = 0.97\n",
            "Epoch 3: Average Episodic Reward = 0.03 | Value Loss = -0.00 | Policy Loss = 0.92\n",
            "Epoch 4: Average Episodic Reward = 0.03 | Value Loss = -0.00 | Policy Loss = 0.90\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = 0.02 | Policy Loss = 0.97\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_19/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 20\n",
            "\n",
            "Start time: 02-05-2021 15:50:12\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_20/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = 0.10 | Policy Loss = 0.86\n",
            "Epoch 2: Average Episodic Reward = 0.05 | Value Loss = -0.02 | Policy Loss = 0.99\n",
            "Epoch 3: Average Episodic Reward = 0.03 | Value Loss = 0.05 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.03 | Value Loss = 0.06 | Policy Loss = 0.98\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = 0.00 | Policy Loss = 0.87\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_20/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 21\n",
            "\n",
            "Start time: 02-05-2021 15:50:12\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_21/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.08 | Value Loss = -0.04 | Policy Loss = 0.89\n",
            "Epoch 2: Average Episodic Reward = 0.10 | Value Loss = -0.07 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.06 | Value Loss = 0.07 | Policy Loss = 0.89\n",
            "Epoch 4: Average Episodic Reward = 0.07 | Value Loss = 0.04 | Policy Loss = 0.96\n",
            "Epoch 5: Average Episodic Reward = 0.05 | Value Loss = 0.03 | Policy Loss = 0.92\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_21/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 22\n",
            "\n",
            "Start time: 02-05-2021 15:50:13\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_22/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.07 | Value Loss = 0.02 | Policy Loss = 1.02\n",
            "Epoch 2: Average Episodic Reward = 0.05 | Value Loss = -0.04 | Policy Loss = 0.90\n",
            "Epoch 3: Average Episodic Reward = 0.08 | Value Loss = -0.05 | Policy Loss = 0.93\n",
            "Epoch 4: Average Episodic Reward = 0.06 | Value Loss = 0.01 | Policy Loss = 0.88\n",
            "Epoch 5: Average Episodic Reward = 0.05 | Value Loss = 0.06 | Policy Loss = 0.92\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_22/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 23\n",
            "\n",
            "Start time: 02-05-2021 15:50:13\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_23/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = 0.03 | Policy Loss = 0.86\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = 0.02 | Policy Loss = 0.99\n",
            "Epoch 3: Average Episodic Reward = 0.03 | Value Loss = 0.00 | Policy Loss = 0.99\n",
            "Epoch 4: Average Episodic Reward = 0.06 | Value Loss = -0.07 | Policy Loss = 0.97\n",
            "Epoch 5: Average Episodic Reward = 0.01 | Value Loss = 0.04 | Policy Loss = 0.91\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_23/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 24\n",
            "\n",
            "Start time: 02-05-2021 15:50:14\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_24/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = -0.06 | Policy Loss = 0.96\n",
            "Epoch 2: Average Episodic Reward = 0.03 | Value Loss = -0.03 | Policy Loss = 0.94\n",
            "Epoch 3: Average Episodic Reward = 0.03 | Value Loss = -0.03 | Policy Loss = 0.93\n",
            "Epoch 4: Average Episodic Reward = 0.03 | Value Loss = -0.01 | Policy Loss = 0.93\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = 0.00 | Policy Loss = 0.96\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_24/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 25\n",
            "\n",
            "Start time: 02-05-2021 15:50:14\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_25/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.04 | Value Loss = -0.01 | Policy Loss = 0.96\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = 0.06 | Policy Loss = 0.85\n",
            "Epoch 3: Average Episodic Reward = 0.03 | Value Loss = -0.02 | Policy Loss = 0.97\n",
            "Epoch 4: Average Episodic Reward = 0.03 | Value Loss = -0.02 | Policy Loss = 0.97\n",
            "Epoch 5: Average Episodic Reward = 0.04 | Value Loss = -0.02 | Policy Loss = 0.98\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_25/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 26\n",
            "\n",
            "Start time: 02-05-2021 15:50:15\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_26/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = 0.00 | Policy Loss = 0.94\n",
            "Epoch 2: Average Episodic Reward = 0.03 | Value Loss = 0.00 | Policy Loss = 0.94\n",
            "Epoch 3: Average Episodic Reward = 0.04 | Value Loss = 0.00 | Policy Loss = 0.94\n",
            "Epoch 4: Average Episodic Reward = 0.04 | Value Loss = -0.00 | Policy Loss = 0.96\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = -0.01 | Policy Loss = 0.97\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_26/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 27\n",
            "\n",
            "Start time: 02-05-2021 15:50:15\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_27/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.08 | Value Loss = 0.00 | Policy Loss = 0.94\n",
            "Epoch 2: Average Episodic Reward = 0.08 | Value Loss = 0.00 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.06 | Value Loss = 0.01 | Policy Loss = 0.98\n",
            "Epoch 4: Average Episodic Reward = 0.07 | Value Loss = 0.00 | Policy Loss = 0.93\n",
            "Epoch 5: Average Episodic Reward = 0.08 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_27/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 28\n",
            "\n",
            "Start time: 02-05-2021 15:50:16\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_28/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = -0.01 | Policy Loss = 0.93\n",
            "Epoch 2: Average Episodic Reward = 0.06 | Value Loss = -0.00 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.06 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.04 | Value Loss = 0.02 | Policy Loss = 0.94\n",
            "Epoch 5: Average Episodic Reward = 0.07 | Value Loss = -0.01 | Policy Loss = 0.94\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_28/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 29\n",
            "\n",
            "Start time: 02-05-2021 15:50:16\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_29/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.05 | Value Loss = -0.02 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.04 | Value Loss = 0.01 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.05 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.05 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.05 | Value Loss = 0.00 | Policy Loss = 0.94\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_29/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 30\n",
            "\n",
            "Start time: 02-05-2021 15:50:17\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_30/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.01 | Value Loss = 0.02 | Policy Loss = 0.92\n",
            "Epoch 2: Average Episodic Reward = 0.01 | Value Loss = 0.08 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.01 | Value Loss = 0.02 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = -0.02 | Policy Loss = 0.91\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = -0.04 | Policy Loss = 0.94\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_30/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 31\n",
            "\n",
            "Start time: 02-05-2021 15:50:17\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_31/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.09 | Value Loss = -0.06 | Policy Loss = 0.96\n",
            "Epoch 2: Average Episodic Reward = 0.07 | Value Loss = -0.01 | Policy Loss = 0.97\n",
            "Epoch 3: Average Episodic Reward = 0.08 | Value Loss = -0.01 | Policy Loss = 0.96\n",
            "Epoch 4: Average Episodic Reward = 0.09 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.10 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_31/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 32\n",
            "\n",
            "Start time: 02-05-2021 15:50:18\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_32/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.01 | Value Loss = 0.06 | Policy Loss = 0.91\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = -0.02 | Policy Loss = 0.99\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = -0.04 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = -0.02 | Policy Loss = 0.96\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_32/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 33\n",
            "\n",
            "Start time: 02-05-2021 15:50:19\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_33/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.09 | Value Loss = -0.01 | Policy Loss = 0.94\n",
            "Epoch 2: Average Episodic Reward = 0.09 | Value Loss = 0.01 | Policy Loss = 0.94\n",
            "Epoch 3: Average Episodic Reward = 0.08 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.10 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.08 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_33/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 34\n",
            "\n",
            "Start time: 02-05-2021 15:50:19\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_34/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.05 | Value Loss = 0.05 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.05 | Value Loss = 0.03 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.06 | Value Loss = -0.01 | Policy Loss = 0.91\n",
            "Epoch 4: Average Episodic Reward = 0.04 | Value Loss = 0.01 | Policy Loss = 0.90\n",
            "Epoch 5: Average Episodic Reward = 0.05 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_34/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 35\n",
            "\n",
            "Start time: 02-05-2021 15:50:20\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_35/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.06 | Value Loss = -0.01 | Policy Loss = 0.92\n",
            "Epoch 2: Average Episodic Reward = 0.08 | Value Loss = -0.03 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.07 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.07 | Value Loss = 0.02 | Policy Loss = 0.94\n",
            "Epoch 5: Average Episodic Reward = 0.06 | Value Loss = 0.03 | Policy Loss = 1.01\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_35/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 36\n",
            "\n",
            "Start time: 02-05-2021 15:50:20\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_36/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.06 | Value Loss = -0.03 | Policy Loss = 0.94\n",
            "Epoch 2: Average Episodic Reward = 0.06 | Value Loss = -0.01 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.05 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.07 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.06 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_36/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 37\n",
            "\n",
            "Start time: 02-05-2021 15:50:21\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_37/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.06 | Value Loss = 0.00 | Policy Loss = 0.92\n",
            "Epoch 2: Average Episodic Reward = 0.07 | Value Loss = 0.01 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.08 | Value Loss = -0.01 | Policy Loss = 0.94\n",
            "Epoch 4: Average Episodic Reward = 0.06 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.07 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_37/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 38\n",
            "\n",
            "Start time: 02-05-2021 15:50:21\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_38/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.08 | Value Loss = 0.00 | Policy Loss = 0.96\n",
            "Epoch 2: Average Episodic Reward = 0.10 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.07 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.08 | Value Loss = 0.01 | Policy Loss = 0.94\n",
            "Epoch 5: Average Episodic Reward = 0.10 | Value Loss = -0.01 | Policy Loss = 0.94\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_38/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 39\n",
            "\n",
            "Start time: 02-05-2021 15:50:22\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_39/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = 0.03 | Policy Loss = 0.89\n",
            "Epoch 3: Average Episodic Reward = 0.03 | Value Loss = -0.00 | Policy Loss = 0.96\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = -0.02 | Policy Loss = 0.94\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = 0.04 | Policy Loss = 0.89\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_39/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 40\n",
            "\n",
            "Start time: 02-05-2021 15:50:22\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_40/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.13 | Value Loss = -0.04 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.09 | Value Loss = -0.02 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.10 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.11 | Value Loss = -0.00 | Policy Loss = 0.96\n",
            "Epoch 5: Average Episodic Reward = 0.09 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_40/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 41\n",
            "\n",
            "Start time: 02-05-2021 15:50:23\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_41/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.05 | Value Loss = 0.01 | Policy Loss = 0.93\n",
            "Epoch 2: Average Episodic Reward = 0.04 | Value Loss = 0.01 | Policy Loss = 0.97\n",
            "Epoch 3: Average Episodic Reward = 0.03 | Value Loss = 0.04 | Policy Loss = 0.99\n",
            "Epoch 4: Average Episodic Reward = 0.04 | Value Loss = -0.02 | Policy Loss = 0.92\n",
            "Epoch 5: Average Episodic Reward = 0.04 | Value Loss = -0.03 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_41/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 42\n",
            "\n",
            "Start time: 02-05-2021 15:50:24\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_42/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.01 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.01 | Value Loss = 0.00 | Policy Loss = 0.94\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = -0.00 | Policy Loss = 0.94\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = -0.01 | Policy Loss = 0.94\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_42/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 43\n",
            "\n",
            "Start time: 02-05-2021 15:50:24\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_43/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.06 | Value Loss = -0.00 | Policy Loss = 0.94\n",
            "Epoch 2: Average Episodic Reward = 0.06 | Value Loss = 0.02 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.05 | Value Loss = 0.02 | Policy Loss = 0.96\n",
            "Epoch 4: Average Episodic Reward = 0.06 | Value Loss = -0.02 | Policy Loss = 0.96\n",
            "Epoch 5: Average Episodic Reward = 0.05 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_43/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 44\n",
            "\n",
            "Start time: 02-05-2021 15:50:25\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_44/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.04 | Value Loss = -0.01 | Policy Loss = 0.96\n",
            "Epoch 2: Average Episodic Reward = 0.04 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.04 | Value Loss = 0.00 | Policy Loss = 0.93\n",
            "Epoch 4: Average Episodic Reward = 0.03 | Value Loss = -0.00 | Policy Loss = 0.96\n",
            "Epoch 5: Average Episodic Reward = 0.04 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_44/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 45\n",
            "\n",
            "Start time: 02-05-2021 15:50:25\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_45/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.09 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.09 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.09 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.09 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.09 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_45/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 46\n",
            "\n",
            "Start time: 02-05-2021 15:50:26\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_46/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.04 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.06 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.04 | Value Loss = 0.01 | Policy Loss = 0.93\n",
            "Epoch 4: Average Episodic Reward = 0.05 | Value Loss = 0.01 | Policy Loss = 0.94\n",
            "Epoch 5: Average Episodic Reward = 0.04 | Value Loss = 0.03 | Policy Loss = 0.91\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_46/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 47\n",
            "\n",
            "Start time: 02-05-2021 15:50:26\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_47/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.09 | Value Loss = -0.03 | Policy Loss = 0.92\n",
            "Epoch 2: Average Episodic Reward = 0.08 | Value Loss = -0.01 | Policy Loss = 0.98\n",
            "Epoch 3: Average Episodic Reward = 0.09 | Value Loss = 0.02 | Policy Loss = 0.94\n",
            "Epoch 4: Average Episodic Reward = 0.06 | Value Loss = 0.03 | Policy Loss = 0.88\n",
            "Epoch 5: Average Episodic Reward = 0.08 | Value Loss = -0.02 | Policy Loss = 0.87\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_47/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 48\n",
            "\n",
            "Start time: 02-05-2021 15:50:27\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_48/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = 0.02 | Policy Loss = 1.03\n",
            "Epoch 2: Average Episodic Reward = 0.01 | Value Loss = -0.04 | Policy Loss = 0.88\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = -0.03 | Policy Loss = 0.94\n",
            "Epoch 4: Average Episodic Reward = 0.01 | Value Loss = -0.00 | Policy Loss = 0.96\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = -0.00 | Policy Loss = 0.96\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_48/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 49\n",
            "\n",
            "Start time: 02-05-2021 15:50:27\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_49/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.09 | Value Loss = -0.02 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.10 | Value Loss = -0.01 | Policy Loss = 0.93\n",
            "Epoch 3: Average Episodic Reward = 0.06 | Value Loss = 0.03 | Policy Loss = 0.98\n",
            "Epoch 4: Average Episodic Reward = 0.07 | Value Loss = -0.01 | Policy Loss = 0.97\n",
            "Epoch 5: Average Episodic Reward = 0.11 | Value Loss = -0.03 | Policy Loss = 0.96\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_49/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 50\n",
            "\n",
            "Start time: 02-05-2021 15:50:28\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_50/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.07 | Value Loss = -0.02 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.09 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.08 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.08 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.07 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_50/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 51\n",
            "\n",
            "Start time: 02-05-2021 15:50:28\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_51/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.08 | Value Loss = -0.00 | Policy Loss = 0.94\n",
            "Epoch 2: Average Episodic Reward = 0.05 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.07 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.07 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.09 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_51/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 52\n",
            "\n",
            "Start time: 02-05-2021 15:50:29\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_52/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = 0.01 | Policy Loss = 0.96\n",
            "Epoch 2: Average Episodic Reward = 0.03 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 0.92\n",
            "Epoch 4: Average Episodic Reward = 0.03 | Value Loss = -0.01 | Policy Loss = 0.93\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = 0.02 | Policy Loss = 0.96\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_52/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 53\n",
            "\n",
            "Start time: 02-05-2021 15:50:30\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_53/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.11 | Value Loss = -0.03 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.12 | Value Loss = -0.02 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.09 | Value Loss = -0.00 | Policy Loss = 0.96\n",
            "Epoch 4: Average Episodic Reward = 0.08 | Value Loss = 0.00 | Policy Loss = 0.94\n",
            "Epoch 5: Average Episodic Reward = 0.09 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_53/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 54\n",
            "\n",
            "Start time: 02-05-2021 15:50:30\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_54/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.08 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.07 | Value Loss = -0.00 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.07 | Value Loss = 0.00 | Policy Loss = 0.96\n",
            "Epoch 4: Average Episodic Reward = 0.07 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.07 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_54/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 55\n",
            "\n",
            "Start time: 02-05-2021 15:50:31\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_55/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.06 | Value Loss = 0.04 | Policy Loss = 0.89\n",
            "Epoch 2: Average Episodic Reward = 0.05 | Value Loss = 0.02 | Policy Loss = 0.99\n",
            "Epoch 3: Average Episodic Reward = 0.07 | Value Loss = -0.03 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.06 | Value Loss = -0.01 | Policy Loss = 0.96\n",
            "Epoch 5: Average Episodic Reward = 0.08 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_55/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 56\n",
            "\n",
            "Start time: 02-05-2021 15:50:31\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_56/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.10 | Value Loss = 0.00 | Policy Loss = 0.94\n",
            "Epoch 2: Average Episodic Reward = 0.08 | Value Loss = 0.01 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.11 | Value Loss = 0.00 | Policy Loss = 0.93\n",
            "Epoch 4: Average Episodic Reward = 0.12 | Value Loss = -0.02 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.12 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_56/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 57\n",
            "\n",
            "Start time: 02-05-2021 15:50:32\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_57/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = 0.04 | Policy Loss = 0.91\n",
            "Epoch 2: Average Episodic Reward = 0.03 | Value Loss = 0.00 | Policy Loss = 0.92\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.03 | Value Loss = -0.00 | Policy Loss = 1.01\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = -0.02 | Policy Loss = 0.93\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_57/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 58\n",
            "\n",
            "Start time: 02-05-2021 15:50:32\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_58/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.06 | Value Loss = -0.03 | Policy Loss = 0.92\n",
            "Epoch 2: Average Episodic Reward = 0.06 | Value Loss = -0.01 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.05 | Value Loss = 0.01 | Policy Loss = 0.96\n",
            "Epoch 4: Average Episodic Reward = 0.05 | Value Loss = 0.01 | Policy Loss = 0.98\n",
            "Epoch 5: Average Episodic Reward = 0.05 | Value Loss = -0.02 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_58/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 59\n",
            "\n",
            "Start time: 02-05-2021 15:50:33\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_59/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.10 | Value Loss = -0.02 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.10 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.09 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.08 | Value Loss = 0.02 | Policy Loss = 0.90\n",
            "Epoch 5: Average Episodic Reward = 0.10 | Value Loss = -0.01 | Policy Loss = 0.96\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_59/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 60\n",
            "\n",
            "Start time: 02-05-2021 15:50:33\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_60/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.08 | Value Loss = 0.01 | Policy Loss = 0.97\n",
            "Epoch 2: Average Episodic Reward = 0.12 | Value Loss = -0.02 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.07 | Value Loss = 0.01 | Policy Loss = 0.96\n",
            "Epoch 4: Average Episodic Reward = 0.11 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.11 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_60/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 61\n",
            "\n",
            "Start time: 02-05-2021 15:50:34\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_61/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.10 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.09 | Value Loss = 0.00 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.10 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.11 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.11 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_61/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 62\n",
            "\n",
            "Start time: 02-05-2021 15:50:35\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_62/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.04 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.04 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = 0.02 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.03 | Value Loss = -0.01 | Policy Loss = 0.96\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_62/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 63\n",
            "\n",
            "Start time: 02-05-2021 15:50:35\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_63/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.08 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.09 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.08 | Value Loss = 0.00 | Policy Loss = 0.94\n",
            "Epoch 4: Average Episodic Reward = 0.08 | Value Loss = -0.00 | Policy Loss = 0.94\n",
            "Epoch 5: Average Episodic Reward = 0.07 | Value Loss = 0.01 | Policy Loss = 0.94\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_63/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 64\n",
            "\n",
            "Start time: 02-05-2021 15:50:36\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_64/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.11 | Value Loss = -0.01 | Policy Loss = 0.93\n",
            "Epoch 2: Average Episodic Reward = 0.12 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.12 | Value Loss = 0.01 | Policy Loss = 0.96\n",
            "Epoch 4: Average Episodic Reward = 0.12 | Value Loss = -0.01 | Policy Loss = 0.94\n",
            "Epoch 5: Average Episodic Reward = 0.11 | Value Loss = 0.00 | Policy Loss = 0.96\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_64/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 65\n",
            "\n",
            "Start time: 02-05-2021 15:50:36\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_65/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.08 | Value Loss = 0.02 | Policy Loss = 0.93\n",
            "Epoch 2: Average Episodic Reward = 0.13 | Value Loss = -0.02 | Policy Loss = 0.93\n",
            "Epoch 3: Average Episodic Reward = 0.10 | Value Loss = 0.01 | Policy Loss = 0.92\n",
            "Epoch 4: Average Episodic Reward = 0.10 | Value Loss = 0.01 | Policy Loss = 0.98\n",
            "Epoch 5: Average Episodic Reward = 0.10 | Value Loss = 0.00 | Policy Loss = 0.97\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_65/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 66\n",
            "\n",
            "Start time: 02-05-2021 15:50:37\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_66/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.12 | Value Loss = -0.02 | Policy Loss = 0.94\n",
            "Epoch 2: Average Episodic Reward = 0.10 | Value Loss = 0.01 | Policy Loss = 0.97\n",
            "Epoch 3: Average Episodic Reward = 0.10 | Value Loss = -0.00 | Policy Loss = 0.96\n",
            "Epoch 4: Average Episodic Reward = 0.12 | Value Loss = -0.01 | Policy Loss = 0.94\n",
            "Epoch 5: Average Episodic Reward = 0.09 | Value Loss = 0.01 | Policy Loss = 0.94\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_66/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 67\n",
            "\n",
            "Start time: 02-05-2021 15:50:37\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_67/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.06 | Value Loss = 0.01 | Policy Loss = 0.94\n",
            "Epoch 2: Average Episodic Reward = 0.05 | Value Loss = 0.04 | Policy Loss = 0.91\n",
            "Epoch 3: Average Episodic Reward = 0.06 | Value Loss = -0.01 | Policy Loss = 0.98\n",
            "Epoch 4: Average Episodic Reward = 0.06 | Value Loss = -0.02 | Policy Loss = 0.94\n",
            "Epoch 5: Average Episodic Reward = 0.04 | Value Loss = 0.02 | Policy Loss = 0.94\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_67/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 68\n",
            "\n",
            "Start time: 02-05-2021 15:50:38\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_68/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.12 | Value Loss = -0.04 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.09 | Value Loss = -0.02 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.08 | Value Loss = 0.01 | Policy Loss = 0.94\n",
            "Epoch 4: Average Episodic Reward = 0.10 | Value Loss = 0.01 | Policy Loss = 0.91\n",
            "Epoch 5: Average Episodic Reward = 0.12 | Value Loss = -0.02 | Policy Loss = 0.96\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_68/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 69\n",
            "\n",
            "Start time: 02-05-2021 15:50:38\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_69/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.11 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.10 | Value Loss = 0.01 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.09 | Value Loss = 0.00 | Policy Loss = 0.94\n",
            "Epoch 4: Average Episodic Reward = 0.11 | Value Loss = 0.00 | Policy Loss = 0.96\n",
            "Epoch 5: Average Episodic Reward = 0.10 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_69/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 70\n",
            "\n",
            "Start time: 02-05-2021 15:50:39\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_70/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.14 | Value Loss = -0.01 | Policy Loss = 0.94\n",
            "Epoch 2: Average Episodic Reward = 0.09 | Value Loss = 0.03 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.10 | Value Loss = 0.02 | Policy Loss = 1.00\n",
            "Epoch 4: Average Episodic Reward = 0.14 | Value Loss = -0.03 | Policy Loss = 0.97\n",
            "Epoch 5: Average Episodic Reward = 0.13 | Value Loss = -0.02 | Policy Loss = 0.99\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_70/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 71\n",
            "\n",
            "Start time: 02-05-2021 15:50:39\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_71/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.07 | Value Loss = 0.03 | Policy Loss = 0.93\n",
            "Epoch 2: Average Episodic Reward = 0.08 | Value Loss = 0.01 | Policy Loss = 0.94\n",
            "Epoch 3: Average Episodic Reward = 0.12 | Value Loss = -0.03 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.07 | Value Loss = 0.05 | Policy Loss = 1.00\n",
            "Epoch 5: Average Episodic Reward = 0.08 | Value Loss = 0.00 | Policy Loss = 0.94\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_71/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 72\n",
            "\n",
            "Start time: 02-05-2021 15:50:40\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_72/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = -0.03 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.03 | Value Loss = -0.01 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.03 | Value Loss = -0.00 | Policy Loss = 0.94\n",
            "Epoch 4: Average Episodic Reward = 0.03 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 0.96\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_72/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 73\n",
            "\n",
            "Start time: 02-05-2021 15:50:41\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_73/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.08 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.10 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.07 | Value Loss = 0.01 | Policy Loss = 0.96\n",
            "Epoch 4: Average Episodic Reward = 0.06 | Value Loss = 0.00 | Policy Loss = 0.94\n",
            "Epoch 5: Average Episodic Reward = 0.08 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_73/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 74\n",
            "\n",
            "Start time: 02-05-2021 15:50:41\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_74/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = 0.00 | Policy Loss = 0.94\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = -0.01 | Policy Loss = 0.93\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_74/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 75\n",
            "\n",
            "Start time: 02-05-2021 15:50:42\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00917b4d50>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_train/Trial_75/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.12 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.13 | Value Loss = 0.00 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.12 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.13 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.12 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_train/Trial_75/PPO_TabularEnv.pt\n",
            "\n",
            "Loaded model parameters from ./logs/RL2_Exeriment_4/meta_train/Trial_10/PPO_TabularEnv.pt\n",
            "Start time: 02-05-2021 15:50:42\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f004cda6810>, 'epochs': 400, 'episodes_per_epoch': 64, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_4/meta_test', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.06 | Value Loss = -0.03 | Policy Loss = 0.92\n",
            "Epoch 40: Average Episodic Reward = 0.08 | Value Loss = 0.00 | Policy Loss = 0.94\n",
            "Epoch 80: Average Episodic Reward = 0.08 | Value Loss = 0.01 | Policy Loss = 0.94\n",
            "Epoch 120: Average Episodic Reward = 0.10 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 160: Average Episodic Reward = 0.12 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 200: Average Episodic Reward = 0.11 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 240: Average Episodic Reward = 0.13 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 280: Average Episodic Reward = 0.13 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 320: Average Episodic Reward = 0.14 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 360: Average Episodic Reward = 0.14 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 400: Average Episodic Reward = 0.14 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 1009 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_4/meta_test/PPO_TabularEnv.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "VBJOohhxaiZX",
        "outputId": "3015b9e1-8c35-4a99-8691-6a8948e0a76c"
      },
      "source": [
        "# Plot all rewards\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.title('Train Rewards')\n",
        "plt.plot(ema(vanilla_ppo_train_rewards, alpha=0.95))\n",
        "plt.plot(ema(rl2_10_trials_rewards, alpha=0.95))\n",
        "plt.plot(ema(rl2_25_trials_rewards, alpha=0.95))\n",
        "plt.plot(ema(rl2_50_trials_rewards, alpha=0.95))\n",
        "plt.plot(ema(rl2_75_trials_rewards, alpha=0.95))\n",
        "plt.legend(['PPO GRUPolicy', 'n = 10', 'n = 25', 'n = 50', 'n = 75'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Rewards')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Rewards')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P8RqNKhlnmw"
      },
      "source": [
        "### Experiments 2 : Changing the number of episodes per trial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_t1qXeXcmrf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b9401fa-7b99-48d9-cbea-9e8a716bf59b"
      },
      "source": [
        "random.seed(10)\n",
        "np.random.seed(10)\n",
        "torch.manual_seed(10)\n",
        "\n",
        "rl2_4 = RL2(log_dir='./logs/RL2_Experiment_5')\n",
        "rl2_4.meta_train(episodes_per_epoch=2)\n",
        "rl2_10_ep_per_mdp_rewards = rl2_4.meta_test()\n",
        "\n",
        "rl2_5 = RL2(log_dir='./logs/RL2_Experiment_6')\n",
        "rl2_5.meta_train(episodes_per_epoch=5)\n",
        "rl2_25_ep_per_mdp_rewards = rl2_5.meta_test()\n",
        "\n",
        "rl2_6 = RL2(log_dir='./logs/RL2_Experiment_7')\n",
        "rl2_6.meta_train(episodes_per_epoch=10)\n",
        "rl2_50_ep_per_mdp_rewards = rl2_6.meta_test()\n",
        "\n",
        "rl2_7 = RL2(log_dir='./logs/RL2_Exeriment_8')\n",
        "rl2_7.meta_train(episodes_per_epoch=15)\n",
        "rl2_75_ep_per_mdp_rewards = rl2_7.meta_test()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "TRIAL NUMBER : 1\n",
            "\n",
            "Start time: 02-05-2021 16:08:24\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f003a7acdd0>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_5/meta_train/Trial_1/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.04 | Value Loss = -0.08 | Policy Loss = 1.01\n",
            "Epoch 2: Average Episodic Reward = 0.05 | Value Loss = -0.06 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.05 | Value Loss = 0.00 | Policy Loss = 0.93\n",
            "Epoch 4: Average Episodic Reward = 0.03 | Value Loss = -0.01 | Policy Loss = 1.01\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = 0.04 | Policy Loss = 1.00\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_5/meta_train/Trial_1/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 2\n",
            "\n",
            "Start time: 02-05-2021 16:08:24\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f003a7acdd0>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_5/meta_train/Trial_2/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = 0.05 | Policy Loss = 0.97\n",
            "Epoch 2: Average Episodic Reward = 0.04 | Value Loss = 0.00 | Policy Loss = 0.91\n",
            "Epoch 3: Average Episodic Reward = 0.04 | Value Loss = -0.04 | Policy Loss = 1.00\n",
            "Epoch 4: Average Episodic Reward = 0.05 | Value Loss = -0.02 | Policy Loss = 1.02\n",
            "Epoch 5: Average Episodic Reward = 0.04 | Value Loss = 0.00 | Policy Loss = 0.91\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_5/meta_train/Trial_2/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 3\n",
            "\n",
            "Start time: 02-05-2021 16:08:25\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f003a7acdd0>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_5/meta_train/Trial_3/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 0.88\n",
            "Epoch 2: Average Episodic Reward = 0.01 | Value Loss = 0.06 | Policy Loss = 0.44\n",
            "Epoch 3: Average Episodic Reward = 0.03 | Value Loss = -0.07 | Policy Loss = 0.89\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = -0.05 | Policy Loss = 0.82\n",
            "Epoch 5: Average Episodic Reward = 0.01 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_5/meta_train/Trial_3/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 4\n",
            "\n",
            "Start time: 02-05-2021 16:08:26\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f003a7acdd0>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_5/meta_train/Trial_4/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.05 | Value Loss = -0.03 | Policy Loss = 0.99\n",
            "Epoch 2: Average Episodic Reward = 0.03 | Value Loss = 0.05 | Policy Loss = 0.84\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = 0.04 | Policy Loss = 0.93\n",
            "Epoch 4: Average Episodic Reward = 0.06 | Value Loss = -0.09 | Policy Loss = 0.93\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = 0.04 | Policy Loss = 0.99\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_5/meta_train/Trial_4/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 5\n",
            "\n",
            "Start time: 02-05-2021 16:08:26\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f003a7acdd0>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_5/meta_train/Trial_5/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.00 | Value Loss = 0.27 | Policy Loss = 0.49\n",
            "Epoch 2: Average Episodic Reward = 0.03 | Value Loss = 0.03 | Policy Loss = 0.87\n",
            "Epoch 3: Average Episodic Reward = 0.00 | Value Loss = 0.08 | Policy Loss = 0.46\n",
            "Epoch 4: Average Episodic Reward = 0.00 | Value Loss = 0.07 | Policy Loss = 0.02\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = -0.04 | Policy Loss = 0.90\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_5/meta_train/Trial_5/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 6\n",
            "\n",
            "Start time: 02-05-2021 16:08:27\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f003a7acdd0>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_5/meta_train/Trial_6/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.00 | Value Loss = -0.14 | Policy Loss = 0.96\n",
            "Epoch 2: Average Episodic Reward = 0.00 | Value Loss = -0.09 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.01 | Value Loss = -0.04 | Policy Loss = 0.86\n",
            "Epoch 4: Average Episodic Reward = 0.00 | Value Loss = -0.03 | Policy Loss = 0.93\n",
            "Epoch 5: Average Episodic Reward = 0.00 | Value Loss = -0.01 | Policy Loss = 0.88\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_5/meta_train/Trial_6/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 7\n",
            "\n",
            "Start time: 02-05-2021 16:08:27\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f003a7acdd0>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_5/meta_train/Trial_7/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.01 | Value Loss = 0.09 | Policy Loss = 0.48\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = -0.04 | Policy Loss = 0.89\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = -0.08 | Policy Loss = 0.82\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 0.79\n",
            "Epoch 5: Average Episodic Reward = 0.01 | Value Loss = 0.12 | Policy Loss = 1.05\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_5/meta_train/Trial_7/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 8\n",
            "\n",
            "Start time: 02-05-2021 16:08:28\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f003a7acdd0>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_5/meta_train/Trial_8/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.04 | Value Loss = -0.19 | Policy Loss = 0.91\n",
            "Epoch 2: Average Episodic Reward = 0.03 | Value Loss = -0.11 | Policy Loss = 0.97\n",
            "Epoch 3: Average Episodic Reward = 0.04 | Value Loss = -0.06 | Policy Loss = 0.97\n",
            "Epoch 4: Average Episodic Reward = 0.04 | Value Loss = -0.03 | Policy Loss = 0.99\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = -0.04 | Policy Loss = 0.97\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_5/meta_train/Trial_8/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 9\n",
            "\n",
            "Start time: 02-05-2021 16:08:28\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f003a7acdd0>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_5/meta_train/Trial_9/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.01 | Value Loss = 0.08 | Policy Loss = 0.97\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = 0.05 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.01 | Value Loss = 0.03 | Policy Loss = 0.94\n",
            "Epoch 4: Average Episodic Reward = 0.01 | Value Loss = 0.02 | Policy Loss = 0.91\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_5/meta_train/Trial_9/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 10\n",
            "\n",
            "Start time: 02-05-2021 16:08:29\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f003a7acdd0>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_5/meta_train/Trial_10/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = -0.03 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.00 | Value Loss = 0.15 | Policy Loss = 0.53\n",
            "Epoch 3: Average Episodic Reward = 0.03 | Value Loss = 0.04 | Policy Loss = 0.93\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = -0.02 | Policy Loss = 0.97\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = 0.04 | Policy Loss = 0.46\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_5/meta_train/Trial_10/PPO_TabularEnv.pt\n",
            "\n",
            "Loaded model parameters from ./logs/RL2_Experiment_5/meta_train/Trial_10/PPO_TabularEnv.pt\n",
            "Start time: 02-05-2021 16:08:29\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f003a75f890>, 'epochs': 400, 'episodes_per_epoch': 64, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_5/meta_test', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.05 | Value Loss = -0.05 | Policy Loss = 0.95\n",
            "Epoch 40: Average Episodic Reward = 0.06 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 80: Average Episodic Reward = 0.09 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 120: Average Episodic Reward = 0.10 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 160: Average Episodic Reward = 0.12 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 200: Average Episodic Reward = 0.12 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 240: Average Episodic Reward = 0.13 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 280: Average Episodic Reward = 0.14 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 320: Average Episodic Reward = 0.14 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 360: Average Episodic Reward = 0.14 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 400: Average Episodic Reward = 0.15 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 1051 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_5/meta_test/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 1\n",
            "\n",
            "Start time: 02-05-2021 16:26:01\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00902a25d0>, 'epochs': 5, 'episodes_per_epoch': 5, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_6/meta_train/Trial_1/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = -0.20 | Policy Loss = 1.02\n",
            "Epoch 2: Average Episodic Reward = 0.03 | Value Loss = -0.15 | Policy Loss = 0.98\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = -0.07 | Policy Loss = 0.96\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = 0.00 | Policy Loss = 0.89\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = -0.07 | Policy Loss = 0.90\n",
            "\n",
            "Training Completed in 1 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_6/meta_train/Trial_1/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 2\n",
            "\n",
            "Start time: 02-05-2021 16:26:02\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00902a25d0>, 'epochs': 5, 'episodes_per_epoch': 5, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_6/meta_train/Trial_2/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.01 | Value Loss = 0.02 | Policy Loss = 0.98\n",
            "Epoch 2: Average Episodic Reward = 0.01 | Value Loss = -0.03 | Policy Loss = 0.98\n",
            "Epoch 3: Average Episodic Reward = 0.01 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.01 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.01 | Value Loss = -0.03 | Policy Loss = 0.96\n",
            "\n",
            "Training Completed in 1 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_6/meta_train/Trial_2/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 3\n",
            "\n",
            "Start time: 02-05-2021 16:26:03\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00902a25d0>, 'epochs': 5, 'episodes_per_epoch': 5, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_6/meta_train/Trial_3/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = 0.03 | Policy Loss = 0.87\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = 0.00 | Policy Loss = 0.92\n",
            "Epoch 3: Average Episodic Reward = 0.03 | Value Loss = -0.09 | Policy Loss = 0.96\n",
            "Epoch 4: Average Episodic Reward = 0.03 | Value Loss = -0.04 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = 0.05 | Policy Loss = 0.70\n",
            "\n",
            "Training Completed in 1 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_6/meta_train/Trial_3/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 4\n",
            "\n",
            "Start time: 02-05-2021 16:26:05\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00902a25d0>, 'epochs': 5, 'episodes_per_epoch': 5, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_6/meta_train/Trial_4/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = -0.02 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.03 | Value Loss = -0.04 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.04 | Value Loss = -0.04 | Policy Loss = 0.96\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = 0.00 | Policy Loss = 0.94\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = 0.02 | Policy Loss = 0.79\n",
            "\n",
            "Training Completed in 1 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_6/meta_train/Trial_4/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 5\n",
            "\n",
            "Start time: 02-05-2021 16:26:06\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00902a25d0>, 'epochs': 5, 'episodes_per_epoch': 5, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_6/meta_train/Trial_5/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = -0.01 | Policy Loss = 0.97\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.03 | Value Loss = 0.00 | Policy Loss = 0.96\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = -0.00 | Policy Loss = 0.94\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = -0.01 | Policy Loss = 0.94\n",
            "\n",
            "Training Completed in 1 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_6/meta_train/Trial_5/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 6\n",
            "\n",
            "Start time: 02-05-2021 16:26:07\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00902a25d0>, 'epochs': 5, 'episodes_per_epoch': 5, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_6/meta_train/Trial_6/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.04 | Value Loss = 0.01 | Policy Loss = 0.94\n",
            "Epoch 2: Average Episodic Reward = 0.04 | Value Loss = -0.01 | Policy Loss = 0.97\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = 0.00 | Policy Loss = 0.94\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = -0.00 | Policy Loss = 0.92\n",
            "\n",
            "Training Completed in 1 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_6/meta_train/Trial_6/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 7\n",
            "\n",
            "Start time: 02-05-2021 16:26:08\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00902a25d0>, 'epochs': 5, 'episodes_per_epoch': 5, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_6/meta_train/Trial_7/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.01 | Value Loss = 0.02 | Policy Loss = 0.78\n",
            "Epoch 2: Average Episodic Reward = 0.01 | Value Loss = -0.01 | Policy Loss = 0.76\n",
            "Epoch 3: Average Episodic Reward = 0.01 | Value Loss = -0.00 | Policy Loss = 0.93\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.01 | Value Loss = 0.02 | Policy Loss = 0.92\n",
            "\n",
            "Training Completed in 1 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_6/meta_train/Trial_7/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 8\n",
            "\n",
            "Start time: 02-05-2021 16:26:09\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00902a25d0>, 'epochs': 5, 'episodes_per_epoch': 5, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_6/meta_train/Trial_8/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = -0.02 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.05 | Value Loss = -0.04 | Policy Loss = 0.93\n",
            "Epoch 3: Average Episodic Reward = 0.04 | Value Loss = 0.01 | Policy Loss = 0.94\n",
            "Epoch 4: Average Episodic Reward = 0.04 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.05 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 1 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_6/meta_train/Trial_8/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 9\n",
            "\n",
            "Start time: 02-05-2021 16:26:11\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00902a25d0>, 'epochs': 5, 'episodes_per_epoch': 5, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_6/meta_train/Trial_9/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = 0.01 | Policy Loss = 0.94\n",
            "Epoch 2: Average Episodic Reward = 0.03 | Value Loss = -0.00 | Policy Loss = 0.93\n",
            "Epoch 3: Average Episodic Reward = 0.04 | Value Loss = -0.03 | Policy Loss = 0.92\n",
            "Epoch 4: Average Episodic Reward = 0.03 | Value Loss = -0.01 | Policy Loss = 0.99\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = 0.01 | Policy Loss = 0.97\n",
            "\n",
            "Training Completed in 1 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_6/meta_train/Trial_9/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 10\n",
            "\n",
            "Start time: 02-05-2021 16:26:12\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f00902a25d0>, 'epochs': 5, 'episodes_per_epoch': 5, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_6/meta_train/Trial_10/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.05 | Value Loss = -0.02 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.06 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.04 | Value Loss = 0.00 | Policy Loss = 0.96\n",
            "Epoch 4: Average Episodic Reward = 0.04 | Value Loss = 0.00 | Policy Loss = 0.94\n",
            "Epoch 5: Average Episodic Reward = 0.04 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 1 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_6/meta_train/Trial_10/PPO_TabularEnv.pt\n",
            "\n",
            "Loaded model parameters from ./logs/RL2_Experiment_6/meta_train/Trial_10/PPO_TabularEnv.pt\n",
            "Start time: 02-05-2021 16:26:13\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f003a7d47d0>, 'epochs': 400, 'episodes_per_epoch': 64, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_6/meta_test', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.04 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "Epoch 40: Average Episodic Reward = 0.05 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 80: Average Episodic Reward = 0.06 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 120: Average Episodic Reward = 0.07 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 160: Average Episodic Reward = 0.07 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 200: Average Episodic Reward = 0.08 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 240: Average Episodic Reward = 0.08 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 280: Average Episodic Reward = 0.08 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 320: Average Episodic Reward = 0.08 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 360: Average Episodic Reward = 0.08 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 400: Average Episodic Reward = 0.08 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 1063 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_6/meta_test/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 1\n",
            "\n",
            "Start time: 02-05-2021 16:43:57\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f003a75f650>, 'epochs': 5, 'episodes_per_epoch': 10, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_7/meta_train/Trial_1/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = 0.10 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.03 | Value Loss = 0.04 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.04 | Value Loss = 0.01 | Policy Loss = 0.96\n",
            "Epoch 4: Average Episodic Reward = 0.03 | Value Loss = 0.00 | Policy Loss = 0.96\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = 0.01 | Policy Loss = 0.93\n",
            "\n",
            "Training Completed in 2 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_7/meta_train/Trial_1/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 2\n",
            "\n",
            "Start time: 02-05-2021 16:43:59\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f003a75f650>, 'epochs': 5, 'episodes_per_epoch': 10, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_7/meta_train/Trial_2/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.05 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.06 | Value Loss = -0.02 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.04 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.06 | Value Loss = -0.01 | Policy Loss = 0.96\n",
            "Epoch 5: Average Episodic Reward = 0.04 | Value Loss = 0.02 | Policy Loss = 0.96\n",
            "\n",
            "Training Completed in 2 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_7/meta_train/Trial_2/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 3\n",
            "\n",
            "Start time: 02-05-2021 16:44:02\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f003a75f650>, 'epochs': 5, 'episodes_per_epoch': 10, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_7/meta_train/Trial_3/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.01 | Value Loss = 0.03 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.01 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.01 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.01 | Value Loss = 0.00 | Policy Loss = 0.96\n",
            "Epoch 5: Average Episodic Reward = 0.01 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 2 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_7/meta_train/Trial_3/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 4\n",
            "\n",
            "Start time: 02-05-2021 16:44:04\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f003a75f650>, 'epochs': 5, 'episodes_per_epoch': 10, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_7/meta_train/Trial_4/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.01 | Value Loss = 0.04 | Policy Loss = 0.74\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = 0.02 | Policy Loss = 0.85\n",
            "Epoch 3: Average Episodic Reward = 0.03 | Value Loss = -0.02 | Policy Loss = 0.92\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = -0.01 | Policy Loss = 0.94\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 0.80\n",
            "\n",
            "Training Completed in 2 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_7/meta_train/Trial_4/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 5\n",
            "\n",
            "Start time: 02-05-2021 16:44:06\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f003a75f650>, 'epochs': 5, 'episodes_per_epoch': 10, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_7/meta_train/Trial_5/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.06 | Value Loss = -0.07 | Policy Loss = 0.82\n",
            "Epoch 2: Average Episodic Reward = 0.14 | Value Loss = 0.03 | Policy Loss = 0.76\n",
            "Epoch 3: Average Episodic Reward = 0.04 | Value Loss = 0.00 | Policy Loss = 0.85\n",
            "Epoch 4: Average Episodic Reward = 0.14 | Value Loss = -0.00 | Policy Loss = 0.86\n",
            "Epoch 5: Average Episodic Reward = 0.05 | Value Loss = -0.03 | Policy Loss = 0.92\n",
            "\n",
            "Training Completed in 2 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_7/meta_train/Trial_5/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 6\n",
            "\n",
            "Start time: 02-05-2021 16:44:08\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f003a75f650>, 'epochs': 5, 'episodes_per_epoch': 10, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_7/meta_train/Trial_6/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.04 | Value Loss = 0.00 | Policy Loss = 0.87\n",
            "Epoch 2: Average Episodic Reward = 0.05 | Value Loss = -0.05 | Policy Loss = 0.90\n",
            "Epoch 3: Average Episodic Reward = 0.05 | Value Loss = -0.07 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.06 | Value Loss = -0.00 | Policy Loss = 0.94\n",
            "Epoch 5: Average Episodic Reward = 0.05 | Value Loss = 0.04 | Policy Loss = 0.83\n",
            "\n",
            "Training Completed in 2 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_7/meta_train/Trial_6/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 7\n",
            "\n",
            "Start time: 02-05-2021 16:44:11\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f003a75f650>, 'epochs': 5, 'episodes_per_epoch': 10, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_7/meta_train/Trial_7/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = 0.16 | Policy Loss = 0.77\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = 0.10 | Policy Loss = 0.70\n",
            "Epoch 3: Average Episodic Reward = 0.13 | Value Loss = 0.00 | Policy Loss = 0.76\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = 0.10 | Policy Loss = 0.82\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = -0.01 | Policy Loss = 0.83\n",
            "\n",
            "Training Completed in 2 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_7/meta_train/Trial_7/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 8\n",
            "\n",
            "Start time: 02-05-2021 16:44:13\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f003a75f650>, 'epochs': 5, 'episodes_per_epoch': 10, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_7/meta_train/Trial_8/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.06 | Value Loss = -0.31 | Policy Loss = 1.05\n",
            "Epoch 2: Average Episodic Reward = 0.06 | Value Loss = -0.23 | Policy Loss = 1.00\n",
            "Epoch 3: Average Episodic Reward = 0.05 | Value Loss = -0.13 | Policy Loss = 0.94\n",
            "Epoch 4: Average Episodic Reward = 0.06 | Value Loss = -0.08 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.05 | Value Loss = -0.06 | Policy Loss = 0.96\n",
            "\n",
            "Training Completed in 2 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_7/meta_train/Trial_8/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 9\n",
            "\n",
            "Start time: 02-05-2021 16:44:15\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f003a75f650>, 'epochs': 5, 'episodes_per_epoch': 10, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_7/meta_train/Trial_9/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = 0.04 | Policy Loss = 0.92\n",
            "Epoch 2: Average Episodic Reward = 0.01 | Value Loss = 0.00 | Policy Loss = 0.97\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = 0.03 | Policy Loss = 0.92\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = -0.00 | Policy Loss = 0.93\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 2 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_7/meta_train/Trial_9/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 10\n",
            "\n",
            "Start time: 02-05-2021 16:44:17\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f003a75f650>, 'epochs': 5, 'episodes_per_epoch': 10, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_7/meta_train/Trial_10/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = -0.06 | Policy Loss = 0.94\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = 0.00 | Policy Loss = 0.94\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 0.94\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 2 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_7/meta_train/Trial_10/PPO_TabularEnv.pt\n",
            "\n",
            "Loaded model parameters from ./logs/RL2_Experiment_7/meta_train/Trial_10/PPO_TabularEnv.pt\n",
            "Start time: 02-05-2021 16:44:20\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f003a80e8d0>, 'epochs': 400, 'episodes_per_epoch': 64, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Experiment_7/meta_test', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.06 | Value Loss = 0.09 | Policy Loss = 0.92\n",
            "Epoch 40: Average Episodic Reward = 0.04 | Value Loss = -0.00 | Policy Loss = 0.93\n",
            "Epoch 80: Average Episodic Reward = 0.05 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 120: Average Episodic Reward = 0.05 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "Epoch 160: Average Episodic Reward = 0.06 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "Epoch 200: Average Episodic Reward = 0.07 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 240: Average Episodic Reward = 0.07 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 280: Average Episodic Reward = 0.07 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 320: Average Episodic Reward = 0.08 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 360: Average Episodic Reward = 0.08 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 400: Average Episodic Reward = 0.08 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 1055 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Experiment_7/meta_test/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 1\n",
            "\n",
            "Start time: 02-05-2021 17:01:55\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f003a7ecbd0>, 'epochs': 5, 'episodes_per_epoch': 15, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_8/meta_train/Trial_1/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = -0.08 | Policy Loss = 0.87\n",
            "Epoch 2: Average Episodic Reward = 0.04 | Value Loss = -0.04 | Policy Loss = 0.93\n",
            "Epoch 3: Average Episodic Reward = 0.03 | Value Loss = -0.00 | Policy Loss = 0.90\n",
            "Epoch 4: Average Episodic Reward = 0.04 | Value Loss = -0.04 | Policy Loss = 0.92\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = -0.02 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 3 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_8/meta_train/Trial_1/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 2\n",
            "\n",
            "Start time: 02-05-2021 17:01:58\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f003a7ecbd0>, 'epochs': 5, 'episodes_per_epoch': 15, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_8/meta_train/Trial_2/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.01 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.01 | Value Loss = 0.01 | Policy Loss = 0.94\n",
            "Epoch 3: Average Episodic Reward = 0.01 | Value Loss = -0.01 | Policy Loss = 0.94\n",
            "Epoch 4: Average Episodic Reward = 0.01 | Value Loss = 0.01 | Policy Loss = 0.86\n",
            "Epoch 5: Average Episodic Reward = 0.01 | Value Loss = -0.01 | Policy Loss = 0.93\n",
            "\n",
            "Training Completed in 3 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_8/meta_train/Trial_2/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 3\n",
            "\n",
            "Start time: 02-05-2021 17:02:01\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f003a7ecbd0>, 'epochs': 5, 'episodes_per_epoch': 15, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_8/meta_train/Trial_3/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = -0.03 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = -0.00 | Policy Loss = 0.94\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 0.94\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 0.93\n",
            "\n",
            "Training Completed in 3 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_8/meta_train/Trial_3/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 4\n",
            "\n",
            "Start time: 02-05-2021 17:02:04\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f003a7ecbd0>, 'epochs': 5, 'episodes_per_epoch': 15, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_8/meta_train/Trial_4/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = -0.01 | Policy Loss = 0.91\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 0.93\n",
            "Epoch 3: Average Episodic Reward = 0.02 | Value Loss = 0.03 | Policy Loss = 0.94\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = -0.02 | Policy Loss = 0.85\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = -0.01 | Policy Loss = 0.92\n",
            "\n",
            "Training Completed in 3 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_8/meta_train/Trial_4/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 5\n",
            "\n",
            "Start time: 02-05-2021 17:02:08\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f003a7ecbd0>, 'epochs': 5, 'episodes_per_epoch': 15, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_8/meta_train/Trial_5/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = -0.06 | Policy Loss = 0.89\n",
            "Epoch 2: Average Episodic Reward = 0.03 | Value Loss = -0.01 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.03 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.03 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = -0.01 | Policy Loss = 0.96\n",
            "\n",
            "Training Completed in 3 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_8/meta_train/Trial_5/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 6\n",
            "\n",
            "Start time: 02-05-2021 17:02:11\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f003a7ecbd0>, 'epochs': 5, 'episodes_per_epoch': 15, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_8/meta_train/Trial_6/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.04 | Value Loss = -0.01 | Policy Loss = 0.94\n",
            "Epoch 3: Average Episodic Reward = 0.04 | Value Loss = 0.00 | Policy Loss = 0.93\n",
            "Epoch 4: Average Episodic Reward = 0.04 | Value Loss = 0.01 | Policy Loss = 0.96\n",
            "Epoch 5: Average Episodic Reward = 0.04 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 3 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_8/meta_train/Trial_6/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 7\n",
            "\n",
            "Start time: 02-05-2021 17:02:14\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f003a7ecbd0>, 'epochs': 5, 'episodes_per_epoch': 15, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_8/meta_train/Trial_7/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.04 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.04 | Value Loss = -0.01 | Policy Loss = 0.93\n",
            "Epoch 3: Average Episodic Reward = 0.04 | Value Loss = 0.01 | Policy Loss = 0.93\n",
            "Epoch 4: Average Episodic Reward = 0.04 | Value Loss = 0.01 | Policy Loss = 0.88\n",
            "Epoch 5: Average Episodic Reward = 0.05 | Value Loss = -0.03 | Policy Loss = 0.94\n",
            "\n",
            "Training Completed in 3 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_8/meta_train/Trial_7/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 8\n",
            "\n",
            "Start time: 02-05-2021 17:02:17\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f003a7ecbd0>, 'epochs': 5, 'episodes_per_epoch': 15, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_8/meta_train/Trial_8/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.05 | Value Loss = 0.02 | Policy Loss = 0.92\n",
            "Epoch 2: Average Episodic Reward = 0.06 | Value Loss = -0.02 | Policy Loss = 0.93\n",
            "Epoch 3: Average Episodic Reward = 0.05 | Value Loss = 0.03 | Policy Loss = 0.89\n",
            "Epoch 4: Average Episodic Reward = 0.06 | Value Loss = -0.02 | Policy Loss = 0.93\n",
            "Epoch 5: Average Episodic Reward = 0.07 | Value Loss = -0.03 | Policy Loss = 0.90\n",
            "\n",
            "Training Completed in 3 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_8/meta_train/Trial_8/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 9\n",
            "\n",
            "Start time: 02-05-2021 17:02:21\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f003a7ecbd0>, 'epochs': 5, 'episodes_per_epoch': 15, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_8/meta_train/Trial_9/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.01 | Value Loss = 0.08 | Policy Loss = 0.78\n",
            "Epoch 2: Average Episodic Reward = 0.01 | Value Loss = 0.02 | Policy Loss = 0.89\n",
            "Epoch 3: Average Episodic Reward = 0.01 | Value Loss = -0.00 | Policy Loss = 0.93\n",
            "Epoch 4: Average Episodic Reward = 0.01 | Value Loss = -0.02 | Policy Loss = 0.87\n",
            "Epoch 5: Average Episodic Reward = 0.01 | Value Loss = -0.00 | Policy Loss = 0.92\n",
            "\n",
            "Training Completed in 3 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_8/meta_train/Trial_9/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 10\n",
            "\n",
            "Start time: 02-05-2021 17:02:24\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f003a7ecbd0>, 'epochs': 5, 'episodes_per_epoch': 15, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_8/meta_train/Trial_10/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.04 | Value Loss = -0.07 | Policy Loss = 0.92\n",
            "Epoch 2: Average Episodic Reward = 0.03 | Value Loss = -0.00 | Policy Loss = 0.91\n",
            "Epoch 3: Average Episodic Reward = 0.03 | Value Loss = 0.00 | Policy Loss = 0.97\n",
            "Epoch 4: Average Episodic Reward = 0.03 | Value Loss = -0.02 | Policy Loss = 0.96\n",
            "Epoch 5: Average Episodic Reward = 0.04 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 3 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_8/meta_train/Trial_10/PPO_TabularEnv.pt\n",
            "\n",
            "Loaded model parameters from ./logs/RL2_Exeriment_8/meta_train/Trial_10/PPO_TabularEnv.pt\n",
            "Start time: 02-05-2021 17:02:27\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f003a7bec50>, 'epochs': 400, 'episodes_per_epoch': 64, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/RL2_Exeriment_8/meta_test', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "Epoch 40: Average Episodic Reward = 0.03 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 80: Average Episodic Reward = 0.04 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 120: Average Episodic Reward = 0.04 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 160: Average Episodic Reward = 0.05 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 200: Average Episodic Reward = 0.05 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 240: Average Episodic Reward = 0.05 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 280: Average Episodic Reward = 0.06 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 320: Average Episodic Reward = 0.06 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 360: Average Episodic Reward = 0.06 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 400: Average Episodic Reward = 0.06 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 1065 seconds\n",
            "\n",
            "Saved model parameters to logs/RL2_Exeriment_8/meta_test/PPO_TabularEnv.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJchlfITqZcm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "7211c398-e295-450f-eebb-a3abe0c64ee6"
      },
      "source": [
        "# Plot Rewards\n",
        "\n",
        "plt.title('Train Rewards')\n",
        "plt.plot(ema(vanilla_ppo_train_rewards, alpha=0.95))\n",
        "plt.plot(ema(rl2_10_ep_per_mdp_rewards, alpha=0.95))\n",
        "plt.plot(ema(rl2_25_ep_per_mdp_rewards, alpha=0.95))\n",
        "plt.plot(ema(rl2_50_ep_per_mdp_rewards, alpha=0.95))\n",
        "plt.plot(ema(rl2_75_ep_per_mdp_rewards, alpha=0.95))\n",
        "plt.legend(['PPO GRUPolicy (EPM = 64)', 'RL2 (EPM = 10)', 'RL2 (EPM = 25)', 'RL2 (EPM = 50)', 'RL2 (EPM = 75)'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Rewards')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Rewards')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwmdlGDrudS7"
      },
      "source": [
        "### Ablations\n",
        "\n",
        "Ablations performed:\n",
        "\n",
        "* Retaining the hidden State Across Different Tasks\n",
        "* Using an MLPPolicy instead of a GRUPolicy for RL$^2$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDu0yYOpa4uQ"
      },
      "source": [
        "#### Retaining Hidden State across tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUlv11IyvQGn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "085def37-6c7c-4b13-d3fd-11183ce49240"
      },
      "source": [
        "random.seed(10)\n",
        "np.random.seed(10)\n",
        "torch.manual_seed(10)\n",
        "\n",
        "rl2_no_reset = RL2(log_dir='./logs/Ablation_1/')\n",
        "rl2_no_reset.meta_train(reset_hidden_state=False)\n",
        "rl2_no_reset_rewards = rl2_no_reset.meta_test()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "TRIAL NUMBER : 1\n",
            "\n",
            "Start time: 02-05-2021 17:27:44\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0024797290>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/Ablation_1/meta_train/Trial_1/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.04 | Value Loss = -0.07 | Policy Loss = 0.90\n",
            "Epoch 2: Average Episodic Reward = 0.05 | Value Loss = -0.06 | Policy Loss = 0.85\n",
            "Epoch 3: Average Episodic Reward = 0.03 | Value Loss = 0.02 | Policy Loss = 0.42\n",
            "Epoch 4: Average Episodic Reward = 0.04 | Value Loss = -0.02 | Policy Loss = 0.93\n",
            "Epoch 5: Average Episodic Reward = 0.07 | Value Loss = -0.11 | Policy Loss = 0.91\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/Ablation_1/meta_train/Trial_1/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 2\n",
            "\n",
            "Start time: 02-05-2021 17:27:45\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0024797290>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/Ablation_1/meta_train/Trial_2/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.07 | Value Loss = -0.16 | Policy Loss = 0.97\n",
            "Epoch 2: Average Episodic Reward = 0.08 | Value Loss = -0.09 | Policy Loss = 1.02\n",
            "Epoch 3: Average Episodic Reward = 0.07 | Value Loss = 0.01 | Policy Loss = 1.01\n",
            "Epoch 4: Average Episodic Reward = 0.07 | Value Loss = -0.03 | Policy Loss = 0.94\n",
            "Epoch 5: Average Episodic Reward = 0.06 | Value Loss = -0.00 | Policy Loss = 0.93\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/Ablation_1/meta_train/Trial_2/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 3\n",
            "\n",
            "Start time: 02-05-2021 17:27:45\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0024797290>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/Ablation_1/meta_train/Trial_3/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.00 | Value Loss = 0.18 | Policy Loss = 0.04\n",
            "Epoch 2: Average Episodic Reward = 0.01 | Value Loss = 0.09 | Policy Loss = 0.83\n",
            "Epoch 3: Average Episodic Reward = 0.03 | Value Loss = -0.05 | Policy Loss = 1.00\n",
            "Epoch 4: Average Episodic Reward = 0.03 | Value Loss = -0.02 | Policy Loss = 1.01\n",
            "Epoch 5: Average Episodic Reward = 0.05 | Value Loss = 0.01 | Policy Loss = 0.91\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/Ablation_1/meta_train/Trial_3/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 4\n",
            "\n",
            "Start time: 02-05-2021 17:27:46\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0024797290>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/Ablation_1/meta_train/Trial_4/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.06 | Value Loss = -0.08 | Policy Loss = 0.98\n",
            "Epoch 2: Average Episodic Reward = 0.05 | Value Loss = -0.06 | Policy Loss = 0.98\n",
            "Epoch 3: Average Episodic Reward = 0.07 | Value Loss = 0.00 | Policy Loss = 0.96\n",
            "Epoch 4: Average Episodic Reward = 0.05 | Value Loss = 0.00 | Policy Loss = 0.93\n",
            "Epoch 5: Average Episodic Reward = 0.08 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/Ablation_1/meta_train/Trial_4/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 5\n",
            "\n",
            "Start time: 02-05-2021 17:27:47\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0024797290>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/Ablation_1/meta_train/Trial_5/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.04 | Value Loss = 0.04 | Policy Loss = 0.90\n",
            "Epoch 2: Average Episodic Reward = 0.01 | Value Loss = 0.04 | Policy Loss = 0.50\n",
            "Epoch 3: Average Episodic Reward = 0.01 | Value Loss = 0.06 | Policy Loss = 0.88\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = 0.02 | Policy Loss = 0.90\n",
            "Epoch 5: Average Episodic Reward = 0.04 | Value Loss = -0.07 | Policy Loss = 0.84\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/Ablation_1/meta_train/Trial_5/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 6\n",
            "\n",
            "Start time: 02-05-2021 17:27:47\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0024797290>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/Ablation_1/meta_train/Trial_6/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 1.01\n",
            "Epoch 2: Average Episodic Reward = 0.04 | Value Loss = -0.02 | Policy Loss = 1.00\n",
            "Epoch 3: Average Episodic Reward = 0.00 | Value Loss = 0.06 | Policy Loss = 0.01\n",
            "Epoch 4: Average Episodic Reward = 0.01 | Value Loss = 0.01 | Policy Loss = 0.51\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = -0.03 | Policy Loss = 0.89\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/Ablation_1/meta_train/Trial_6/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 7\n",
            "\n",
            "Start time: 02-05-2021 17:27:48\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0024797290>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/Ablation_1/meta_train/Trial_7/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.05 | Value Loss = -0.04 | Policy Loss = 0.96\n",
            "Epoch 2: Average Episodic Reward = 0.07 | Value Loss = -0.02 | Policy Loss = 0.90\n",
            "Epoch 3: Average Episodic Reward = 0.06 | Value Loss = -0.02 | Policy Loss = 0.97\n",
            "Epoch 4: Average Episodic Reward = 0.04 | Value Loss = 0.01 | Policy Loss = 0.94\n",
            "Epoch 5: Average Episodic Reward = 0.06 | Value Loss = -0.01 | Policy Loss = 0.96\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/Ablation_1/meta_train/Trial_7/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 8\n",
            "\n",
            "Start time: 02-05-2021 17:27:48\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0024797290>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/Ablation_1/meta_train/Trial_8/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.05 | Value Loss = -0.04 | Policy Loss = 0.94\n",
            "Epoch 2: Average Episodic Reward = 0.06 | Value Loss = -0.01 | Policy Loss = 0.97\n",
            "Epoch 3: Average Episodic Reward = 0.04 | Value Loss = 0.03 | Policy Loss = 0.96\n",
            "Epoch 4: Average Episodic Reward = 0.06 | Value Loss = 0.01 | Policy Loss = 0.94\n",
            "Epoch 5: Average Episodic Reward = 0.05 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/Ablation_1/meta_train/Trial_8/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 9\n",
            "\n",
            "Start time: 02-05-2021 17:27:49\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0024797290>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/Ablation_1/meta_train/Trial_9/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.07 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.08 | Value Loss = 0.00 | Policy Loss = 0.94\n",
            "Epoch 3: Average Episodic Reward = 0.08 | Value Loss = 0.00 | Policy Loss = 0.96\n",
            "Epoch 4: Average Episodic Reward = 0.05 | Value Loss = -0.00 | Policy Loss = 0.96\n",
            "Epoch 5: Average Episodic Reward = 0.07 | Value Loss = 0.00 | Policy Loss = 0.96\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/Ablation_1/meta_train/Trial_9/PPO_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 10\n",
            "\n",
            "Start time: 02-05-2021 17:27:49\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0024797290>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/Ablation_1/meta_train/Trial_10/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = 0.00 | Policy Loss = 0.94\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = -0.01 | Policy Loss = 0.92\n",
            "Epoch 3: Average Episodic Reward = 0.01 | Value Loss = 0.00 | Policy Loss = 0.93\n",
            "Epoch 4: Average Episodic Reward = 0.01 | Value Loss = 0.02 | Policy Loss = 0.96\n",
            "Epoch 5: Average Episodic Reward = 0.01 | Value Loss = -0.01 | Policy Loss = 0.89\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/Ablation_1/meta_train/Trial_10/PPO_TabularEnv.pt\n",
            "\n",
            "Loaded model parameters from ./logs/Ablation_1/meta_train/Trial_10/PPO_TabularEnv.pt\n",
            "Start time: 02-05-2021 17:27:50\n",
            "Training model on TabularEnv | Observation Space: Discrete(10) | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPO object at 0x7f0023f26b10>, 'epochs': 400, 'episodes_per_epoch': 64, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/Ablation_1/meta_test', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True, 'TENSORBOARD_LOG': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.04 | Value Loss = -0.05 | Policy Loss = 0.95\n",
            "Epoch 40: Average Episodic Reward = 0.05 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "Epoch 80: Average Episodic Reward = 0.05 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 120: Average Episodic Reward = 0.07 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 160: Average Episodic Reward = 0.07 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 200: Average Episodic Reward = 0.08 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 240: Average Episodic Reward = 0.08 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 280: Average Episodic Reward = 0.09 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 320: Average Episodic Reward = 0.09 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 360: Average Episodic Reward = 0.09 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 400: Average Episodic Reward = 0.10 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 1068 seconds\n",
            "\n",
            "Saved model parameters to logs/Ablation_1/meta_test/PPO_TabularEnv.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "tSDkVQZDL9Ac",
        "outputId": "88d1bd8c-d684-4e94-daf7-e78712db3ff2"
      },
      "source": [
        "# Plot all rewards\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.title('Train Rewards')\n",
        "plt.plot(ema(vanilla_ppo_train_rewards, alpha=0.95))\n",
        "plt.plot(ema(rl2_10_trials_rewards, alpha=0.95))\n",
        "plt.plot(ema(rl2_no_reset_rewards, alpha=0.95))\n",
        "plt.legend(['PPO GRUPolicy', 'Vanilla RL2', 'RL2 No Reset'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Rewards')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Rewards')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lo4FIYQZ_p9B"
      },
      "source": [
        "#### RL2 with MLP Policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f_J6jHF_96y"
      },
      "source": [
        "##### PPO class for using MLP Policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRltgJOEcIXf"
      },
      "source": [
        "import datetime\n",
        "import time\n",
        "from collections import deque, namedtuple\n",
        "from itertools import count\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import wandb\n",
        "!wandb login\n",
        "\n",
        "wandb.init(project='ppo-mlp', config={})\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dtype = torch.double\n",
        "\n",
        "# Hyperparameters\n",
        "EPOCHS = 200\n",
        "EPISODES_PER_EPOCH = 64\n",
        "POLICY_HIDDEN_LAYERS = 32\n",
        "VALUE_HIDDEN_LAYERS = 32\n",
        "N_POLICY_UPDATES = 16\n",
        "N_VALUE_UPDATES = 16\n",
        "GAMMA = 0.99\n",
        "EPSILON = 0.1\n",
        "VALUE_FN_LEARNING_RATE = 1e-3\n",
        "POLICY_LEARNING_RATE = 3e-4\n",
        "MAX_TRAJ_LENGTH = 1000\n",
        "\n",
        "\n",
        "class Trajectory:\n",
        "    def __init__(\n",
        "        self, observations=[], actions=[], rewards=[], dones=[], logits=[],\n",
        "    ):\n",
        "        self.obs = observations\n",
        "        self.a = actions\n",
        "        self.r = rewards\n",
        "        self.d = dones\n",
        "        self.logits = logits\n",
        "        self.len = 0\n",
        "\n",
        "    def add(\n",
        "        self,\n",
        "        obs: torch.Tensor,\n",
        "        a: torch.Tensor,\n",
        "        r: torch.Tensor,\n",
        "        d: torch.Tensor,\n",
        "        logits: torch.Tensor,\n",
        "    ):\n",
        "        self.obs.append(obs)\n",
        "        self.a.append(a)\n",
        "        self.r.append(r)\n",
        "        self.d.append(d)\n",
        "        self.logits.append(logits)\n",
        "        self.len += 1\n",
        "\n",
        "    def disc_r(self, gamma, normalize=False):\n",
        "        disc_rewards = []\n",
        "        r = 0.0\n",
        "        for reward in self.r[::-1]:\n",
        "            r = reward + gamma * r\n",
        "            disc_rewards.insert(0, r)\n",
        "        disc_rewards = torch.tensor(disc_rewards, device=device, dtype=dtype)\n",
        "        if normalize:\n",
        "            disc_rewards = (disc_rewards - disc_rewards.mean()) / (\n",
        "                disc_rewards.std() + np.finfo(np.float32).eps\n",
        "            )\n",
        "        return disc_rewards\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "\n",
        "# Model\n",
        "class PPOMLP:\n",
        "    def __init__(\n",
        "        self,\n",
        "        env,\n",
        "        policy_hidden_layers=POLICY_HIDDEN_LAYERS,\n",
        "        value_hidden_layers=VALUE_HIDDEN_LAYERS,\n",
        "        policy=None,\n",
        "        value=None\n",
        "    ):\n",
        "        self.env = env\n",
        "        self.env_name = 'TabularEnv'\n",
        "        if policy is not None:\n",
        "          self.policy=policy\n",
        "        else:\n",
        "          self.policy = (\n",
        "              nn.Sequential(\n",
        "                  nn.Linear(env.nState, policy_hidden_layers),\n",
        "                  nn.Dropout(p=0.6),\n",
        "                  nn.ReLU(),\n",
        "                  nn.Linear(policy_hidden_layers, env.nAction),\n",
        "              )\n",
        "              .to(device)\n",
        "              .to(dtype)\n",
        "          )\n",
        "\n",
        "        if value is not None:\n",
        "          self.value=value\n",
        "        else:\n",
        "          self.value = (\n",
        "              nn.Sequential(\n",
        "                  nn.Linear(env.nState, value_hidden_layers),\n",
        "                  nn.Dropout(p=0.6),\n",
        "                  nn.ReLU(),\n",
        "                  nn.Linear(value_hidden_layers, 1),\n",
        "              )\n",
        "              .to(device)\n",
        "              .to(dtype)\n",
        "          )\n",
        "\n",
        "    def _update(self, batch, hp, policy_optim, value_optim):\n",
        "        # process batch\n",
        "        obs = [torch.stack(traj.obs)[:-1] for traj in batch]\n",
        "        disc_r = [traj.disc_r(hp[\"gamma\"], normalize=True) for traj in batch]\n",
        "        a = [torch.stack(traj.a) for traj in batch]\n",
        "        with torch.no_grad():\n",
        "            v = [self.value(o) for o in obs]\n",
        "            adv = [disc_r[i] - v[i] for i in range(len(batch))]\n",
        "            old_logits = [torch.stack(traj.logits) for traj in batch]\n",
        "            old_logprobs = [\n",
        "                -F.cross_entropy(old_logits[i], a[i]) for i in range(len(batch))\n",
        "            ]\n",
        "\n",
        "        # update policy\n",
        "        for j in range(hp[\"n_policy_updates\"]):\n",
        "            policy_loss = torch.zeros(1, device=device, dtype=dtype, requires_grad=True)\n",
        "            for i, traj in enumerate(batch):\n",
        "                curr_logits = self.policy(obs[i])\n",
        "                curr_logprobs = -F.cross_entropy(curr_logits, a[i])\n",
        "                ratio = torch.exp(curr_logprobs - old_logprobs[i])\n",
        "                clipped_ratio = torch.clamp(ratio, 1 - hp[\"epsilon\"], 1 + hp[\"epsilon\"])\n",
        "                policy_loss = (\n",
        "                    policy_loss\n",
        "                    + torch.min(ratio * adv[i], clipped_ratio * adv[i]).mean()\n",
        "                )\n",
        "\n",
        "            policy_loss = policy_loss / len(batch)\n",
        "            policy_optim.zero_grad()\n",
        "            policy_loss.backward()\n",
        "            policy_optim.step()\n",
        "\n",
        "        # update value function\n",
        "        for j in range(hp[\"n_value_updates\"]):\n",
        "            value_loss = torch.zeros(1, device=device, dtype=dtype, requires_grad=True)\n",
        "            for i in range(len(batch)):\n",
        "                v = self.value(obs[i]).view(-1)\n",
        "                value_loss = value_loss + F.mse_loss(v, disc_r[i])\n",
        "            value_loss = value_loss / len(batch)\n",
        "            value_optim.zero_grad()\n",
        "            value_loss.backward()\n",
        "            value_optim.step()\n",
        "\n",
        "        return policy_loss.item(), value_loss.item()\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        epochs=EPOCHS,\n",
        "        episodes_per_epoch=EPISODES_PER_EPOCH,\n",
        "        n_value_updates=N_VALUE_UPDATES,\n",
        "        n_policy_updates=N_POLICY_UPDATES,\n",
        "        value_lr=VALUE_FN_LEARNING_RATE,\n",
        "        policy_lr=POLICY_LEARNING_RATE,\n",
        "        gamma=GAMMA,\n",
        "        epsilon=EPSILON,\n",
        "        max_traj_length=MAX_TRAJ_LENGTH,\n",
        "        log_dir=\"./logs/\",\n",
        "        RENDER=False,\n",
        "        PLOT_REWARDS=True,\n",
        "        VERBOSE=False,\n",
        "    ):\n",
        "        \"\"\" Trains both policy and value networks \"\"\"\n",
        "        hp = locals()\n",
        "        start_time = datetime.datetime.now()\n",
        "        print(\n",
        "            f\"Start time: {start_time:%d-%m-%Y %H:%M:%S}\"\n",
        "            f\"\\nTraining model on {self.env_name} | \"\n",
        "            f\"Observation Space: {self.env.nState} | \"\n",
        "            f\"Action Space: {self.env.nAction}\\n\"\n",
        "            f\"Hyperparameters: \\n{hp}\\n\"\n",
        "        )\n",
        "        log_path = Path(log_dir)\n",
        "        log_path.mkdir(parents=True, exist_ok=False)\n",
        "\n",
        "        self.policy.train()\n",
        "        self.value.train()\n",
        "        value_optim = torch.optim.Adam(self.value.parameters(), lr=value_lr)\n",
        "        policy_optim = torch.optim.Adam(self.policy.parameters(), lr=policy_lr)\n",
        "        rewards = []\n",
        "        e = 0\n",
        "\n",
        "        try:\n",
        "            for epoch in range(epochs):\n",
        "\n",
        "                epoch_rewards = []\n",
        "                batch = []\n",
        "\n",
        "                # Sample trajectories\n",
        "                for _ in range(episodes_per_epoch):\n",
        "                    # initialise tracking variables\n",
        "                    obs = self.env.reset()\n",
        "                    obs = torch.tensor(obs, device=device)\n",
        "                    obs = F.one_hot(obs, num_classes=self.env.nState)\n",
        "                    obs = obs.to(dtype)\n",
        "                    traj = Trajectory([obs], [], [], [], [])\n",
        "                    d = False\n",
        "                    e += 1\n",
        "\n",
        "                    # run for single trajectory\n",
        "                    for i in range(max_traj_length):\n",
        "\n",
        "                        a_logits = self.policy(obs)\n",
        "                        a = torch.distributions.Categorical(logits=a_logits).sample()\n",
        "\n",
        "                        obs, r, d, _ = self.env.advance(a.item())\n",
        "\n",
        "                        obs = torch.tensor(obs, device=device)\n",
        "                        obs = F.one_hot(obs, num_classes=self.env.nState)\n",
        "                        obs = obs.to(dtype)\n",
        "                        r = torch.tensor(r, device=device, dtype=dtype)\n",
        "                        traj.add(obs, a, r, d, a_logits)\n",
        "\n",
        "                        if d:\n",
        "                            break\n",
        "\n",
        "                    epoch_rewards.append(sum(traj.r))\n",
        "                    batch.append(traj)\n",
        "\n",
        "                # Update value and policy\n",
        "                p_loss, v_loss = self._update(\n",
        "                    batch, hp, policy_optim, value_optim,\n",
        "                )\n",
        "\n",
        "                # Log rewards and losses\n",
        "                epoch_rewards_tensor = torch.tensor(epoch_rewards, device=device, dtype=dtype)\n",
        "                avg_episode_reward = torch.mean(epoch_rewards_tensor[-episodes_per_epoch:])\n",
        "                rewards.append(avg_episode_reward)\n",
        "\n",
        "                wandb.log({\n",
        "                    \"policy_loss\": p_loss,\n",
        "                    \"value_loss\": v_loss,\n",
        "                    \"reward\": avg_episode_reward,\n",
        "                    \"epoch\": epoch,\n",
        "                })\n",
        "\n",
        "                if VERBOSE and (epoch == 0 or ((epoch + 1) % (epochs / 10)) == 0):\n",
        "                    print(\n",
        "                        f\"Epoch {epoch+1}: Average Episodic Reward = {avg_episode_reward:.2f} |\"\n",
        "                        f\" Value Loss = {p_loss:.2f} |\"\n",
        "                        f\" Policy Loss = {v_loss:.2f}\"\n",
        "                    )\n",
        "\n",
        "        except KeyboardInterrupt as _:\n",
        "            print(\"\\nTraining Interrupted!\\n\")\n",
        "\n",
        "        finally:\n",
        "            print(\n",
        "                f\"\\nTraining Completed in {(datetime.datetime.now() - start_time).seconds} seconds\"\n",
        "            )\n",
        "            self.save(\n",
        "                log_path.joinpath(f\"{self.__class__.__name__}_{self.env_name}.pt\")\n",
        "            )\n",
        "            if PLOT_REWARDS:\n",
        "                plt.plot(rewards)\n",
        "                plt.savefig(\n",
        "                    log_path.joinpath(\n",
        "                        f\"{self.__class__.__name__}_{self.env_name}_reward_plot.png\"\n",
        "                    )\n",
        "                )\n",
        "            return rewards\n",
        "\n",
        "    def save(self, path):\n",
        "        \"\"\" Save model parameters \"\"\"\n",
        "        torch.save(\n",
        "            {\n",
        "                \"policy_state_dict\": self.policy.state_dict(),\n",
        "                \"value_state_dict\": self.value.state_dict(),\n",
        "            },\n",
        "            path,\n",
        "        )\n",
        "        print(f\"\\nSaved model parameters to {path}\")\n",
        "\n",
        "    def load(self, path=None):\n",
        "        \"\"\" Load model parameters \"\"\"\n",
        "        if path is None:\n",
        "            path = f\"./models/{self.__class__.__name__}_{self.env_name}.pt\"\n",
        "        checkpoint = torch.load(path)\n",
        "        self.policy.load_state_dict(checkpoint[\"policy_state_dict\"])\n",
        "        self.value.load_state_dict(checkpoint[\"value_state_dict\"])\n",
        "        print(f\"\\nLoaded model parameters from {path}\")\n",
        "\n",
        "    def eval(self, episodes, render=False):\n",
        "        \"\"\" Evaluates model performance \"\"\"\n",
        "\n",
        "        print(f\"\\nEvaluating model for {episodes} episodes ...\\n\")\n",
        "        start_time = datetime.datetime.now()\n",
        "        self.policy.eval()\n",
        "        rewards = []\n",
        "\n",
        "        for episode in range(episodes):\n",
        "\n",
        "            observation = self.env.reset()\n",
        "            observation = torch.tensor(observation, device=device)\n",
        "            observation = F.one_hot(observation, num_classes=self.env.nState)\n",
        "            observation = observation.to(dtype)\n",
        "            done = False\n",
        "            episode_rewards = []\n",
        "\n",
        "            while not done:\n",
        "\n",
        "                logits = self.policy(observation)\n",
        "                action = torch.distributions.Categorical(logits=logits).sample()\n",
        "                next_observation, reward, done, _ = self.env.advance(action.item())\n",
        "                episode_rewards.append(float(reward))\n",
        "                next_observation = torch.tensor(next_observation, device=device)\n",
        "                next_observation = F.one_hot(next_observation, num_classes=self.env.nState)\n",
        "                next_observation = next_observation.to(dtype)\n",
        "                observation = next_observation\n",
        "\n",
        "            total_episode_reward = sum(episode_rewards)\n",
        "            rewards.append(total_episode_reward)\n",
        "            print(\n",
        "                f\"Episode {episode+1}: Total Episode Reward = {total_episode_reward:.2f}\"\n",
        "            )\n",
        "            rewards.append(total_episode_reward)\n",
        "\n",
        "        print(f\"\\nAverage Reward for an episode = {np.mean(rewards):.2f}\")\n",
        "        print(\n",
        "            f\"Evaluation Completed in {(datetime.datetime.now() - start_time).seconds} seconds\"\n",
        "        )"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeBYRwaVAIz9"
      },
      "source": [
        "##### Running PPO with MLP Policy on river swim environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "qPlhL2LwBBYe",
        "outputId": "d5970218-d471-4809-94b5-8567ea1d11bc"
      },
      "source": [
        "random.seed(10)\n",
        "np.random.seed(10)\n",
        "torch.manual_seed(10)\n",
        "\n",
        "env = make_riverSwim(nState=10, epLen=20)\n",
        "agent = PPOMLP(env)\n",
        "vanilla_ppo_mlp_rewards = agent.train(epochs=500, VERBOSE=True, PLOT_REWARDS=True, log_dir='./logs/ppo_mlp_policy')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start time: 03-05-2021 07:37:20\n",
            "Training model on TabularEnv | Observation Space: 10 | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPOMLP object at 0x7fdc564d4a50>, 'epochs': 500, 'episodes_per_epoch': 64, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/ppo_mlp_policy', 'RENDER': False, 'PLOT_REWARDS': True, 'VERBOSE': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = -0.07 | Policy Loss = 0.95\n",
            "Epoch 50: Average Episodic Reward = 0.11 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 100: Average Episodic Reward = 0.11 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 150: Average Episodic Reward = 0.11 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 200: Average Episodic Reward = 0.11 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 250: Average Episodic Reward = 0.11 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 300: Average Episodic Reward = 0.11 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 350: Average Episodic Reward = 0.11 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 400: Average Episodic Reward = 0.11 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 450: Average Episodic Reward = 0.11 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 500: Average Episodic Reward = 0.11 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 1130 seconds\n",
            "\n",
            "Saved model parameters to logs/ppo_mlp_policy/PPOMLP_TabularEnv.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGLL0GnbeRyu"
      },
      "source": [
        "##### Running RL$^2$ on PPO with MLP Policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHWNpKNaeUT5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "544a41fe-ed30-470b-c9c8-6d5415b0dc03"
      },
      "source": [
        "random.seed(10)\n",
        "np.random.seed(10)\n",
        "torch.manual_seed(10)\n",
        "\n",
        "rl2_mlp = RL2(policy_type='mlp', log_dir='./logs/Ablation_2')\n",
        "rl2_mlp.meta_train(reset_hidden_state=False)\n",
        "rl2_mlp_policy_rewards = rl2_mlp.meta_test()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "TRIAL NUMBER : 1\n",
            "\n",
            "Start time: 03-05-2021 08:10:49\n",
            "Training model on TabularEnv | Observation Space: 10 | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPOMLP object at 0x7fdc5ed03f10>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/Ablation_2/meta_train/Trial_1/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.07 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.06 | Value Loss = -0.03 | Policy Loss = 0.99\n",
            "Epoch 3: Average Episodic Reward = 0.06 | Value Loss = -0.03 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.05 | Value Loss = -0.00 | Policy Loss = 0.92\n",
            "Epoch 5: Average Episodic Reward = 0.05 | Value Loss = -0.00 | Policy Loss = 1.01\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/Ablation_2/meta_train/Trial_1/PPOMLP_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 2\n",
            "\n",
            "Start time: 03-05-2021 08:10:49\n",
            "Training model on TabularEnv | Observation Space: 10 | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPOMLP object at 0x7fdc5ed03f10>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/Ablation_2/meta_train/Trial_2/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.10 | Value Loss = 0.02 | Policy Loss = 0.98\n",
            "Epoch 2: Average Episodic Reward = 0.08 | Value Loss = 0.02 | Policy Loss = 0.97\n",
            "Epoch 3: Average Episodic Reward = 0.05 | Value Loss = -0.04 | Policy Loss = 1.01\n",
            "Epoch 4: Average Episodic Reward = 0.09 | Value Loss = 0.02 | Policy Loss = 0.97\n",
            "Epoch 5: Average Episodic Reward = 0.07 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/Ablation_2/meta_train/Trial_2/PPOMLP_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 3\n",
            "\n",
            "Start time: 03-05-2021 08:10:50\n",
            "Training model on TabularEnv | Observation Space: 10 | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPOMLP object at 0x7fdc5ed03f10>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/Ablation_2/meta_train/Trial_3/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = -0.03 | Policy Loss = 0.91\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.03 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 4: Average Episodic Reward = 0.02 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = -0.02 | Policy Loss = 0.96\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/Ablation_2/meta_train/Trial_3/PPOMLP_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 4\n",
            "\n",
            "Start time: 03-05-2021 08:10:51\n",
            "Training model on TabularEnv | Observation Space: 10 | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPOMLP object at 0x7fdc5ed03f10>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/Ablation_2/meta_train/Trial_4/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.03 | Value Loss = 0.01 | Policy Loss = 0.95\n",
            "Epoch 2: Average Episodic Reward = 0.06 | Value Loss = -0.02 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.04 | Value Loss = 0.01 | Policy Loss = 0.94\n",
            "Epoch 4: Average Episodic Reward = 0.05 | Value Loss = 0.00 | Policy Loss = 0.88\n",
            "Epoch 5: Average Episodic Reward = 0.07 | Value Loss = -0.02 | Policy Loss = 1.00\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/Ablation_2/meta_train/Trial_4/PPOMLP_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 5\n",
            "\n",
            "Start time: 03-05-2021 08:10:51\n",
            "Training model on TabularEnv | Observation Space: 10 | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPOMLP object at 0x7fdc5ed03f10>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/Ablation_2/meta_train/Trial_5/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = 0.14 | Policy Loss = 0.92\n",
            "Epoch 2: Average Episodic Reward = 0.06 | Value Loss = 0.04 | Policy Loss = 0.85\n",
            "Epoch 3: Average Episodic Reward = 0.04 | Value Loss = 0.00 | Policy Loss = 0.49\n",
            "Epoch 4: Average Episodic Reward = 0.08 | Value Loss = -0.08 | Policy Loss = 0.87\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = 0.06 | Policy Loss = 0.83\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/Ablation_2/meta_train/Trial_5/PPOMLP_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 6\n",
            "\n",
            "Start time: 03-05-2021 08:10:52\n",
            "Training model on TabularEnv | Observation Space: 10 | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPOMLP object at 0x7fdc5ed03f10>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/Ablation_2/meta_train/Trial_6/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.01 | Value Loss = -0.07 | Policy Loss = 0.97\n",
            "Epoch 2: Average Episodic Reward = 0.01 | Value Loss = -0.06 | Policy Loss = 0.91\n",
            "Epoch 3: Average Episodic Reward = 0.01 | Value Loss = -0.04 | Policy Loss = 0.97\n",
            "Epoch 4: Average Episodic Reward = 0.01 | Value Loss = 0.03 | Policy Loss = 0.89\n",
            "Epoch 5: Average Episodic Reward = 0.01 | Value Loss = -0.01 | Policy Loss = 0.94\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/Ablation_2/meta_train/Trial_6/PPOMLP_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 7\n",
            "\n",
            "Start time: 03-05-2021 08:10:52\n",
            "Training model on TabularEnv | Observation Space: 10 | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPOMLP object at 0x7fdc5ed03f10>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/Ablation_2/meta_train/Trial_7/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.04 | Value Loss = 0.01 | Policy Loss = 0.92\n",
            "Epoch 2: Average Episodic Reward = 0.03 | Value Loss = 0.02 | Policy Loss = 0.87\n",
            "Epoch 3: Average Episodic Reward = 0.05 | Value Loss = -0.05 | Policy Loss = 0.96\n",
            "Epoch 4: Average Episodic Reward = 0.04 | Value Loss = -0.01 | Policy Loss = 0.82\n",
            "Epoch 5: Average Episodic Reward = 0.03 | Value Loss = 0.08 | Policy Loss = 1.08\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/Ablation_2/meta_train/Trial_7/PPOMLP_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 8\n",
            "\n",
            "Start time: 03-05-2021 08:10:53\n",
            "Training model on TabularEnv | Observation Space: 10 | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPOMLP object at 0x7fdc5ed03f10>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/Ablation_2/meta_train/Trial_8/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.04 | Value Loss = -0.07 | Policy Loss = 0.94\n",
            "Epoch 2: Average Episodic Reward = 0.06 | Value Loss = -0.04 | Policy Loss = 0.95\n",
            "Epoch 3: Average Episodic Reward = 0.06 | Value Loss = -0.00 | Policy Loss = 0.98\n",
            "Epoch 4: Average Episodic Reward = 0.06 | Value Loss = -0.01 | Policy Loss = 0.93\n",
            "Epoch 5: Average Episodic Reward = 0.06 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/Ablation_2/meta_train/Trial_8/PPOMLP_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 9\n",
            "\n",
            "Start time: 03-05-2021 08:10:53\n",
            "Training model on TabularEnv | Observation Space: 10 | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPOMLP object at 0x7fdc5ed03f10>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/Ablation_2/meta_train/Trial_9/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.02 | Value Loss = 0.08 | Policy Loss = 1.00\n",
            "Epoch 2: Average Episodic Reward = 0.02 | Value Loss = 0.01 | Policy Loss = 0.94\n",
            "Epoch 3: Average Episodic Reward = 0.01 | Value Loss = -0.00 | Policy Loss = 0.92\n",
            "Epoch 4: Average Episodic Reward = 0.01 | Value Loss = 0.01 | Policy Loss = 0.92\n",
            "Epoch 5: Average Episodic Reward = 0.02 | Value Loss = 0.02 | Policy Loss = 1.01\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/Ablation_2/meta_train/Trial_9/PPOMLP_TabularEnv.pt\n",
            "\n",
            "TRIAL NUMBER : 10\n",
            "\n",
            "Start time: 03-05-2021 08:10:54\n",
            "Training model on TabularEnv | Observation Space: 10 | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPOMLP object at 0x7fdc5ed03f10>, 'epochs': 5, 'episodes_per_epoch': 2, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/Ablation_2/meta_train/Trial_10/', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.04 | Value Loss = -0.00 | Policy Loss = 0.94\n",
            "Epoch 2: Average Episodic Reward = 0.06 | Value Loss = -0.04 | Policy Loss = 0.96\n",
            "Epoch 3: Average Episodic Reward = 0.03 | Value Loss = 0.01 | Policy Loss = 0.96\n",
            "Epoch 4: Average Episodic Reward = 0.04 | Value Loss = -0.01 | Policy Loss = 0.94\n",
            "Epoch 5: Average Episodic Reward = 0.05 | Value Loss = -0.01 | Policy Loss = 0.98\n",
            "\n",
            "Training Completed in 0 seconds\n",
            "\n",
            "Saved model parameters to logs/Ablation_2/meta_train/Trial_10/PPOMLP_TabularEnv.pt\n",
            "\n",
            "Loaded model parameters from ./logs/Ablation_2/meta_train/Trial_10/PPOMLP_TabularEnv.pt\n",
            "Start time: 03-05-2021 08:10:54\n",
            "Training model on TabularEnv | Observation Space: 10 | Action Space: 2\n",
            "Hyperparameters: \n",
            "{'self': <__main__.PPOMLP object at 0x7fdc4d5baad0>, 'epochs': 400, 'episodes_per_epoch': 64, 'n_value_updates': 16, 'n_policy_updates': 16, 'value_lr': 0.001, 'policy_lr': 0.0003, 'gamma': 0.99, 'epsilon': 0.1, 'max_traj_length': 1000, 'log_dir': './logs/Ablation_2/meta_test', 'RENDER': False, 'PLOT_REWARDS': False, 'VERBOSE': True}\n",
            "\n",
            "Epoch 1: Average Episodic Reward = 0.08 | Value Loss = -0.01 | Policy Loss = 0.95\n",
            "Epoch 40: Average Episodic Reward = 0.15 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 80: Average Episodic Reward = 0.16 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 120: Average Episodic Reward = 0.16 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 160: Average Episodic Reward = 0.16 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 200: Average Episodic Reward = 0.16 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 240: Average Episodic Reward = 0.16 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 280: Average Episodic Reward = 0.16 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 320: Average Episodic Reward = 0.16 | Value Loss = 0.00 | Policy Loss = 0.95\n",
            "Epoch 360: Average Episodic Reward = 0.16 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "Epoch 400: Average Episodic Reward = 0.16 | Value Loss = -0.00 | Policy Loss = 0.95\n",
            "\n",
            "Training Completed in 921 seconds\n",
            "\n",
            "Saved model parameters to logs/Ablation_2/meta_test/PPOMLP_TabularEnv.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BADXIv95Da0D"
      },
      "source": [
        "We compare the performances of PPOGRU, PPOMLP, RL$^2$ on PPOGRU and RL$^2$ on PPOMLP. RL$^2$ on PPOGRU and RL$^2$ on PPOMLP are used with same hyperparameters wrt to the MDP (no. of trials, no. of epochs per trial, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "5je-z87ECGK7",
        "outputId": "67538450-c3d3-408f-c2d5-3a873e69f177"
      },
      "source": [
        "# Plotting rewards\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.title('Training Rewards')\n",
        "plt.plot(ema(vanilla_ppo_mlp_rewards, alpha=0.95))\n",
        "plt.plot(ema(vanilla_ppo_train_rewards, alpha=0.95))\n",
        "plt.plot(ema(rl2_mlp_policy_rewards, alpha=0.95))\n",
        "plt.plot(ema(rl2_10_trials_rewards, alpha=0.95))\n",
        "plt.legend(['PPO MLPPolicy', 'PPO GRUPolicy', 'RL2 MLP Policy', 'RL2 GRU Policy'])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fdc46db74d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqPk9BbBVdYm"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}